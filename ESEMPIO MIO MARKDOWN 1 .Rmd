---
title: "Esercizio 1"
author: "Simone Gervasoni"
date: "2024-01-05"
output:
  html_document:
    theme: paper
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

### (a) 
Per poter disegnare una funzione di probabilità di una mistura di 3 poisson, iniziamo importando tutte le librerie necessarie:

```{r warning=FALSE, message= FALSE }
library(tidyverse)
```
Per prima cosa procediamo a definire i parametri delle 3 poisson tale che $\theta_1 = 0.2 ,\ \theta_2 = 0.08, \ \theta_3 = 0.72$ e il vettore dei mixing weights, che rappresenta la probabilità con cui estraiamo da ogni poisson ,tale che $p = (0.2, 0.08, 0.72)$. 
```{r}
k <- 3
p <- c(0.2, 0.08, 0.72)
theta <- c(2.5, 30.4, 14.3)
```
Il supporto di questa variabile casuale è $S_{Y} = \mathbb{N}$ dunque è discreto ed infinito, ovviamente non possiamo mostrare le probabilità su tutto il supporto quindi formeremo un vettore da 1 a 50 e troveremo le probabilità per tutte le singole mixture components e per la mixture stessa.

```{r}
data_pmf <- tibble(
  x  = c(seq(0, 50, by = 1)),
  pois_2.5 = dpois(x, theta[1]),
  pois_30.4 = dpois(x, theta[2]),
  pois_14.3 = dpois(x, theta[3]),
  mixture = pois_2.5 *p[1] + pois_30.4 *p[2] + pois_14.3 *p[3]
)
data_pmf
```
La colonna mixture del `tibble()` può prendere i valori dalle altre colonne poichè vengono costruite sequenzialmente. Notiamo inoltre che questo tibble è in formato wide visto che una riga dovrebbe corrispondere a ben 4 punti sul grafico bisogna dunque rendere il tibble in un formato tidy. 
```{r}
data_tidy <- data_pmf %>%
  gather(key = "Distribuzione", value = "Probabilità", -x)
data_tidy
```
Ora notiamo che ogni riga del tibble corrisponde ad un'unico punto del grafico ed ha una etichetta `Distribuzione` che indica da quale distribuzione è stato estratto. A questo punto disponiamo di tutto il necessario per procedere alla costruzione del grafico tramite `ggplot`. 

```{r}
ggplot(data_tidy, aes(x = x, y = Probabilità, color = Distribuzione)) +
  #mappiamo la probabilità ad y e il vettore x alle x. Usiamo colori distinti in base alla distribuzione.
  geom_point(size = 1) +  # Punti per tutte le distribuzioni
  geom_point(data = filter(data_tidy, Distribuzione == "mixture"), 
             color = "red", size = 3) + # Accentuiamo la distribuzione della nostra mixture
  geom_line() + # Colleghiamo i punti 
  labs(title = "Distribuzione di probabilità di mixture e mixing components",
       x = "x",
       y = "Probabilità") + # Inseriamo un titolo e i labels per gli assi
  theme_minimal()
```
Questo primo grafico risulta essere leggermente improprio da un punto di vista matematico dato che la nostra mixture è una variabile casuale discreta quindi quella linea che connette i punti non esiste tuttavia rende la forma delle nostre distribuzioni più comprensibile. 
Un grafico più accurato ma meno leggibile è il seguente: 

```{r}
ggplot(data_tidy, aes(x = x, y = Probabilità, color = Distribuzione)) +
  geom_point(size = 1) +  # Punti per tutte le distribuzioni
  geom_point(data = filter(data_tidy, Distribuzione == "mixture"), 
             color = "red", size = 3) + # Accentuiamo la distribuzione della nostra mixture
  geom_segment(aes(xend = x , yend = 0), alpha =0.4, linewidth = 0.5) +
  labs(title = "Distribuzione di probabilità di mixture e mixing components",
       x = "x",
       y = "Probabilità") +
  theme_minimal()
```

### (b)
Ora generiamo un campione di numerosità 500 dalla nostra mixture memorizzando l'informazione sulla loro provenienza.
```{r}
set.seed(431)
n <- 500
Sample <- tibble(
  label = sample(1:k, n, replace = TRUE, prob = p), 
  #prende con reinserimento e probabilità p numeri da 1 a k(3)
  simdata = rpois(n,theta[label] ) 
  #estrae da una poisson con parametro uguale al label-esimo elemento di theta
  )
Sample # le etichette sono note
```

### (c)
Ora dobbiamo implementare e applicare l'algoritmo EM sui dati appena simulati ovviamente togliendo l'etichetta che li identifica, quindi S è ignoto mentre $k=3$.
```{r}
simdata <- Sample[,-1]$simdata # togliamo i labels
```

L'algoritmo prevede come stopping criterium anche l'uso della log likelihood: $ll = \sum_{i=1}^{n}\sum_{j=1}^{k}\log(f(y_i;\theta_j))$, dunque scriviamo una funzione che la calcoli.

```{r}
LogMixLik <- function(p_it, stime, X){
 sum(log(p_it[1]*dpois(X,stime[1])+p_it[2]*dpois(X,stime[2])+p_it[3]*dpois(X,stime[3])))
}
```

Implementiamo la funzione `EM` con 3 argomenti: 
1.`Data` che sono i dati senza labels.
2.`thetam` che sono i valori inizializzati di theta. 
3.`pm` che sono i mixing weights inizializzati.

```{r}
EM <- function(Data, thetam,pm ){ 
  tol <- .Machine$double.eps ^ 0.5 # tolleranza che impostiamo 
  i <- 0
  convergenza <- F
  ll<-0
  while(!convergenza){
    ll.old <-  LogMixLik(pm, thetam , Data) 
    #log verosimiglianze passo m-1 
    stime.old <- thetam #stime passo m-1
    #salviamo sia le stime che la likelihood così possiamo 
    #confrontarle alla fine dell'algoritmo e vedere se sono 
    #soddisfate le condizioni del nostro stopping criterium
    
  # E step - posterior probabilities
  #----------------------------------
    prob.post <- tibble( pk1 = pm[1]*dpois(Data,stime.old[1]),
                  #matrice di posterior probabilities
                 pk2 = pm[2] *dpois(Data ,stime.old[2]),
                 pk3 = pm[3]*dpois(Data ,stime.old[3])) %>% 
        mutate(across(everything()) / rowSums(across(everything())))  
  # La funzione mutate prende tutte le celle e le divide 
  # per la somma della loro riga.
    
  # M step
  #-------    
    pm <- colMeans(prob.post) 
    # massimizzazione di pm con la ll complete 
    for(j in 1:k) {
    thetam[j] <- sum(Data * prob.post[j]) / sum(prob.post[j])
    # massimizzazione di thetam con la ll complete
    }
    ll <- LogMixLik(pm, thetam , Data)
    i = i+1 # conta il numero di iterazioni dell'algoritmo
    convergenza <-( abs(ll-ll.old) < 0.0001 & sum(abs(thetam-stime.old)) < tol )
    #convergenza rimarrà TRUE fino a che entrambe le condizioni saranno realizzate
    }
  return(list(weights = pm, thetas= thetam, prob.post = prob.post, n.iter = i ))
  }

```
L'output della funzione è una lista composta da weights (i pesi finali), thetas (i theta finali), prob.post (le probabilità a posteriori), e n.iter il numero di iterazioni dell'algoritmo. 

Concludiamo immettendo nella funzione le nostre inizializzazioni e i nostri dati senza labels:

```{r}
p_iniz <- c(0.3, 0.1 , 0.6) # starting mixing weights
theta_iniz <- c(3.5, 20 ,10) # starting thetas
out <-EM(simdata, theta_iniz, p_iniz)
tibble(
  theta_predicted = round(out$thetas, 3) ,
  theta_true = theta,
  p_predicted = round(out$weights, 3) ,
  p_true = p,
  N_iter = out$n.iter 
)
```

Come notiamo le previsioni sono molto vicine ai valori veri e l'algoritmo ha raggiunto convergenza in 44 iterazioni. 

### (d)
Per assicurarci che l'algoritmo funzioni dovremmo provare con altri valori iniziali. Ovviamente visto che abbiamo costruito la funzione `EM`, ripetere l'algoritmo sarà molto semplice.
Iniziamo dicendo che l'ordine in cui si danno gli starting value è rilevante rispetto all'output finale dell'algoritmo. Per evitare soluzioni congruenti ma con ordine diverso è necessario impostare negli starting values l'ordine che sappiamo già sia corretto.

L'algoritmo non avendo alcuna informazione su p e con una grande distanza dei $\theta_j$, riesce a convergere alla soluzione corretta.
```{r}
p_1 <- c(0.3, 0.3 , 0.3) # starting mixing weights
theta_1 <- c(0.001, 900 ,40) # starting thetas
out1 <-EM(simdata, theta_1, p_1)
tibble(
  theta_predicted = round(out1$thetas, 3) ,
  theta_true = theta,
  p_predicted = round(out1$weights, 3) ,
  p_true = p,
  N_iter = out1$n.iter)
```

Avviene la stessa cosa quando i $\theta_j$ sono poco distanti:
```{r}
p_2 <- c(0.3, 0.3 , 0.3) # starting mixing weights
theta_2 <- c(0.99, 1.01 ,1) # starting thetas
out2 <-EM(simdata,theta_2, p_2)
tibble(
  theta_predicted = round(out2$thetas, 3) ,
  theta_true = theta,
  p_predicted = round(out2$weights, 3) ,
  p_true = p,
  N_iter = out2$n.iter 
)
```
Qua notiamo che il numero delle iterazione è più alto al caso principale

Riesce a convergere agli $\underline{\eta}$ corretti nell'ordine sbagliato, quando diamo delle informazioni sui mixing weights e i $\theta_j$ hanno un ordine sbagliato, questo succede perchè nell'algoritmo `EM` i mixing weights si aggiornano prima.
```{r}
p_3 <- c(0.2, 0.08 , 0.72) # starting mixing weights
theta_3 <- c(400, 4 , 25) # starting thetas
out3 <-EM(simdata, theta_3, p_3)
tibble(
  theta_predicted = round(out3$thetas, 3) ,
  theta_true = theta,
  p_predicted = round(out3$weights, 3) ,
  p_true = p,
  N_iter = out3$n.iter 
)
```



Non dando all'algoritmo alcuna informazione né sugli starting weights né su i mixing weights, l'algoritmo non converge alla soluzione esatta, si ferma ad un massimo locale.
```{r}
p_4 <- c(0.3, 0.3 , 0.3) # starting mixing weights
theta_4 <- c(1, 1 ,1) # starting thetas
out4 <-EM(simdata, theta_4, p_4)
tibble(
  theta_predicted = round(out4$thetas, 3) ,
  theta_true = theta,
  p_predicted = round(out4$weights, 3) ,
  p_true = p,
  N_iter = out4$n.iter 
)
```

Calcoliamo ora l'entropia dunque moltiplico ogni cella per il logaritmo di se stessa e sommo tutte le celle. Il risultato non dipendende dall'ordine in cui  sommo.
```{r}
EN<-out$prob.post %>%
  mutate(across(everything())*log(across(everything())))%>%
  sum()*(-1)
EN
EN_n <- EN / (log(k)*500)
EN_n
```
La seconda misura di entropia è divisa per l'entropia di una assegnazione casuale, che viene spesso usata come soglia di confronto. Vediamo che la nostra entropia è circa il 6\% di quella casuale, questo indica che il nostro clustering è buono.

Calcoliamo ora l'$R^2$, sapendo che la varianza e la media di una poisson sono uguali a $\theta_j$. Possiamo desumere che la media dei gruppi pesata (media della mixture) e la varianza dei gruppi pesata (varianza whithin) sono la stessa cosa. 
```{r}
y_avg <- var_within <- as.numeric(t(out$weights) %*% out$thetas ) 
# media di mixture e var_within 
var_between <- out$weights %*% (out$thetas - y_avg)^2 
#pesiamo la distanza tra i gruppi per ottenere la var_bewteen
r_sq <- 1 - var_within / (var_between+var_within)
r_sq
```

Procediamo a quantificare l'incertezza dell'assegnazione delle osservazioni ai vari gruppi, questa si misura facendo $1 - \max_{j:1,..,k} \hat z_{ij}$.
Andiamo a vedere riga per riga chi ha il valore maggiore lo sottraiamo ad uno e creiamo una colonna che contiene questa informazione. In fine elenchiamo in maniera descrescente le nostre osservazione dalla più incerta a quella meno incerta. 

```{r}
prob.post_incertezza<- out$prob.post %>%
  rowwise() %>% # operiamo riga per riga
  mutate(incertezza = 1 - max(c_across(everything()))) %>%
  add_column(id = 1:n) %>% 
  # visto che dobbiamo riordinare dobbiamo tenere conto di che osservazione si tratta
  arrange(desc(incertezza)) 

prob.post_incertezza %>% 
  select(incertezza, id) %>%
  print(n = 15)
```
Osserviamo dunque che le prime due sono le più incerte e dopo c'è un pareggio per altre 9 osservazioni, possiamo dire dunque che :

```{r}
non_cert <-prob.post_incertezza %>% 
  filter(incertezza>0.40)%>%
  pull(id) 
cat("Le informazioni più incerte sono la", paste0(non_cert,"esima"),"\n")
```
### (f)
Per poter definire a quale gruppo appartiene ogni osservazione applichiamo un naive bayes classifier cioè per ogni riga evidenziamo la probabilità a posteriori più alta e segnamo questa informazione nella colonna`assigned_label`, dopo la confrontiamo con `Sample$label` che contiene i veri labels.
```{r}
prob.post_2<-out$prob.post %>%
  rowwise() %>% # calcolo riga per riga
  mutate(assigned_label = which.max(c_across(everything()))) #controllo qual'è il massimo dei 3

sum(prob.post_2$assigned_label == Sample$label) / length(Sample$label) # calcolo la percentuale
```

Il nostro algoritmo assegna il label corettamente 96,6\% delle volte. 


