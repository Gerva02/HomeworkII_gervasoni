---
title: <center> **Classificazione della salute del feto** </center>
author: <center> Bonanomi Gervasoni Mahhadi </center>
date: <center> 16 Gennaio 2024 </center>
output: 
  html_document:
    self_contained : TRUE
    toc : TRUE
    toc_float: TRUE
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(comment = NA)

```
```{r warning=FALSE, message= FALSE,echo=FALSE }
library(tidyverse)
library(mclust)
library(Rmixmod)
library(GGally)
library(caret)
```
<br><br>
<center> 
#### *Abstract*
</center>
<div style="text-align: justify;">
La lettura delle cardiotografie CTGs, che sono registrazioni grafiche (tracciati) della frequenza cardiaca fetale durante la gravidanza, è soggetta a interpretazioni differenti quando viene eseguita da diversi operatori sanitari. 
Si tratta di una fase essenziale per valutare la salute del feto durante la gravidanza e il travaglio. Essa infatti influenza le decisioni sull'intervento del medico che, a seconda di come viene valutato il feto, opterà ad esempio per la somministrazione di ossigeno alla madre, o in casi più gravi, procederà con un taglio cesareo d'emergenza.

L'obiettivo di questa nostra analisi sarà costruire dei modelli in grado di prevedere con una buona accuratezza se il feto è Normale, Sospetto o Patologico.
Utilizzeremo diverse metodologie di classificazione per valutare e costruire il modello che più si adatta ai nostri dati. A seguito di una breve analisi esplorativa verranno applicate in primis delle tecniche di clustering e successivamente una serie di modelli di classificazione di tipo EDDA o MDA supportate da tecniche di oversampling e undersampling. 
<div style="text-align: justify;">
### 1) Introduzione
La riduzione della mortalità neonatale fa parte dei principi cardine dell'Agenda 2030 per lo sviluppo sostenibile. Sottoscritta nel 2015 dai governi dei 193 membri delle Nazioni Unite, fra gli obiettivi che si prefigge vi è quello di porre fine alle morti prevenibili di neonati e bambini sotto i 5 anni di età. In particolare viene richiesto che tutti i paesi dovranno cercare di ridurre la mortalità neonatale ad almeno 12 per ogni 1000 abitanti nati vivi. Ad oggi però sono più di 60 gli stati lontani da questo traguardo e 2.3 milioni di neonati muoiono ogni anno.
Di questi 904 400 neonati sopravvivono solo pochi giorni a causa di complicanze alla nascita, e oltre un milione sono quelli che muoiono durante il parto. 
La maggior parte di questi decessi sono causati dalla scarsa qualità dell’assistenza prima e durante il parto. Il miglioramento dei servizi sanitari e il potenziamento dei supporti informatici a costi accessibili,quindi, sono punti chiave per arginare queste tragedie.<br><br>
Fatte queste premesse, non si può non considerare la cardiotografia computerizzata CTGc vista la posizione di primaria importanza che occupa nel fornire un ausilio al monitoraggio della salute del feto. Essa infatti è stata introdotta con l’obiettivo di ridurre i falsi positivi e la soggettività dell’interpretazione poichè fornisce un’interpretazione oggettiva della CTG consentendo la comunicazione di numeri piuttosto che di opinioni. Negli anni infatti ci si è resi conto che i risultati pratici conseguiti dall’uso «di massa» della cardiotocografia sono stati di gran lunga inferiori alle aspettative a causa della carenza di criteri univoci di interpretazione e alla scarsa preparazione degli operatori sanitari. <br><br>
Grazie alla cardiotografia computerizzata,invece, non solo si riesce ad ottenere una migliore interpretazione del cardiotogramma, ma sfruttando la grande mole di dati raccolti, si è diffusa la sperimentazione di numerosi algoritmi di machine learning.
Nel progetto seguente ci siamo cimentati nel svilupparne alcuni coi quali identificare con maggior precisione quali sono i parametri che più di altri caratterizzano la salute del feto oltrechè prevedere con minore margine di errore i quadri clinici che richiedono un particolare occhio di riguardo. <br><br>

### 2) Materiali
I dati di interesse provengono dal dataset **fetal_health**, esportabile gratuitamente dal link citato a fine paragrafo. La raccolta di questi dati è stata fatta mediante SisPorto, un programma di analisi automatizzata del tracciato cardiotografico che registra e riporta graficamente, l'attività elettrica del cuore e delle contrazioni uterine. Su queste calcola poi una serie di metriche che vengono valutate durante il monitoraggio cardiotografico del feto, così come indicato dalla FIGO (Federazione Internazionale di Ginecologia e Ostetricia).
Tramite queste rilevazioni si è ottenuto un dataset con 2126 osservazioni di cardiotografie fetali suddivise per 21 variabili a cui si somma l'etichetta di classe *fetal_health* rappresentata dalle seguenti modalità: 1,2,3 che indicano rispettivamente uno stato di salute del feto Normale, Sospetto, Patologico. 




Per completezza di informazione segue una tabella che riporta sinteticamente la descrizione e le modalità dell variabili:


VARIABILE     | DESCRIZIONE  | MODALITÀ 
------------- | -------------| ---------
baseline_value| Frequenza cardiaca fetale di base (FHR) | Valore in Bpm
accelerations|Numero di accelerazioni cardiache al secondo | Valore in Hz
fetal_movement|Numero di accelerazioni fetali al secondo| Valore in Hz                
uterine_contractions| Numero di contrazioni uterine al secondo| Valore in Hz
light_decelerations|Numero di decelerazioni leggere al secondo| Valore in Hz            
severe_decelerations|Numero di decelerazioni gravi al secondo| Valore in Hz          
prolongued_decelerations|Numero di decelerazioni prolungate al secondo|Valore in Hz | abnormal_short_term_variability|Percentuale di tempo con variabilità a breve termine anomala|Valore in Percentuale|
mean_value_of_short_term_variability| Media della variabilità a breve termine|Valore in Millisecondi|  
percentage_of_time_with_abnormal_long_term...| Percentuale di tempo con variabilità a lungo termine anomala |Valore in Percentuale|                      
mean_value_of_long_term_variability| Media della variabilità a lungo termine| Valore in Millisecondi|   
histogram_width| Larghezza dell'istogramma della FHR| Valore in Bpm|                       
histogram_min| Valore minimo dell'istogramma della FHR| Valore in Bpm|                      
histogram_max| Valore massimo dell'istogramma della FHR|Valore in Bpm| 
histogram_number_of_peaks| Numero di picchi nell'istogramma della FHR| Valore numerico (conteggio)|
histogram_number_of_zeroes| Numero di zeri nell'istogramma della FHR| Valore numerico (conteggio)|
histogram_mode | Moda dell'istogramma della FHR |Valore in Bpm|
histogram_mean| Valore medio dell'istogramma della FHR| Valore in Bpm|             
histogram_median| Mediana dell'istogramma della FHR| Valore in Bpm |               
histogram_variance| Varianza dell'istogramma della FHR| Valore in Bpm^2|
histogram_tendency| Asimmetria dell'istogramma della FHR  | **-1**:sinistra **0**:simmetrica **1**:destra |
fetal_health| Salute fetale (etichetta di classe)|**1**:Sano **2**:Sospetto **3**:Patologico |                     
                 
Il dataset non presenta valori mancanti. Valutando la distribuzione delle variabili abbiamo visto come quelle di conteggio naturalmente seguono una distribuzione assimilabile a quella di Poisson di conseguenza si è optato per escluderle dalle successive analisi assieme alle variabili che dimostravano una pronunciata asimmetria. È stata esclusa anche la variabile *histogram_tendency* poichè fattoriale. 
Da un'analisi preliminare dell'istogramma delle frequenze relative condizionato per classi si evince come la classe dei sani è predominante:

```{r, echo=FALSE,fig.align='center', fig.width=5,fig.height=3.5}
fetal_Health <- tibble(read.csv("fetal_health.csv")) %>%
  mutate_at(vars(fetal_health),as.factor) # non è elegante da migliorare
levels(fetal_Health$fetal_health) <- c("Normale","Sospetto","Patologico")
fetal_Health <- fetal_Health %>%
  select(-c(severe_decelerations,
            histogram_number_of_zeroes,
            histogram_number_of_peaks,
            histogram_variance,
            histogram_tendency, 
            percentage_of_time_with_abnormal_long_term_variability, 
            accelerations, 
            fetal_movement,
            prolongued_decelerations))

etichette<-fetal_Health$fetal_health

fetal_Health %>%
ggplot(aes(x=fetal_health,
                y= after_stat(count)/sum(after_stat(count)))) + 
  geom_bar(aes(fill = fetal_health), color="black") +         
  labs(title = "Istogramma frequenza relativa delle classi",x="Condizione del feto", y="Frequenza Relativa")+
theme(plot.title = element_text(hjust = 0.5,face = "bold", size = 14,margin = margin(10, 0, 20, 0)))+  # hjust Imposta l'allineamento orizzontale al centro
          # margin Imposta i margini intorno al grafico
  scale_fill_manual(values = c("Normale" = "#66FF66", "Sospetto" = "dodgerblue3", "Patologico" = "red"))
```
Ulteriori dettagli sul dataset sono presenti al seguente link: [Fetal Health Classification-Kaggle](https://www.kaggle.com/datasets/andrewmvd/fetal-health-classification/data>) 

### 3) Analisi esplorativa
In prima analisi abbiamo deciso di analizzare la correlazione mediante ggpairs. Se ne è dedotto che le variabili *histogram_mean*, *histogram_median* e *histogram_mode* sono fortemente correlate. Si nota come le variabili anche se distinte per fetal_health sono multimodali quindi successivamente ci serviremo della MDA. Inoltre essendo dati presi da macchine, quindi "data entry",è improbabile la presenza di outliers.

Procediamo ora con l'analisi delle componenti principali. Anzitutto, visto l'ordine di grandezza differente fra le variabili abbiamo eseguito una standardizzazione. Con la PCA abbiamo ottenuto che 4 componenti coprono l'80% della variabilità. A questo punto abbiamo selezionato le prime 4 variabili in base ai loadings (cioè a quanto peso ha la singola variabile all'interno della componente principale).
Dopodichè abbiamo fatto un ggpairs delle variabili selezionate. Dal grafico si nota che
le ultime 2 variabili selezionate *mean_value_of_long_term_variability* e *uterine_contractions* non rispettano la normalità e la simmetria in tutti i gruppi.
```{r, echo=FALSE}
n <- nrow(fetal_Health)
#analisi delle componenti principali: 
pca <- fetal_Health%>%
  select(-fetal_health)%>%
  princomp(cor=T) 
k <- ncol(fetal_Health)
#selezioniamo le prime 4 variabili in base ai loadings (quanto peso ha la singola 
#variabile all'interno della componente principale)
(load_vars <- names(fetal_Health)[apply(pca$loadings[,1:4], 2, function(x) which(x**2==max(x**2)))])

```
Fatte le dovute analisi, per il clustering abbiamo optato per il dataset con le sole variabili selezionate tramite pca (non servono le etichette per il clustering).###DA COMMENTARE####
```{r,echo=FALSE,fig.align='center', fig.width=6.5,fig.height=4}
#dataset per model-based clustering:
fetal_Health_EM<-fetal_Health%>%
  select(all_of(load_vars)) #dataset solo con le variabili selezionate tramite pca (non servono le etichette per il clustering)

#dataset per model-based classification
fetal_Health_classification <-fetal_Health%>%
  select(all_of(load_vars),fetal_health)

fetal_Health_viz <- fetal_Health_classification %>%
  gather(key = "Variable", value = "Value", -fetal_health)

# Create a facetted box plot using ggplot2
ggplot(fetal_Health_viz, aes(x = fetal_health, y = Value, fill =fetal_health )) +
  geom_boxplot() +
  facet_wrap(~Variable, scales = "free_y", ncol = 2) +
  labs(x = "Species",
       y = "Value",
       fill = "Variable") +
  theme_minimal() +
labs( title = "BoxPlot variabili mantenute")+
theme(plot.title = element_text(hjust = 0.5,face = "bold", size = 14,margin = margin(10, 0, 20, 0)))+  # hjust Imposta l'allineamento orizzontale al centro
          # margin Imposta i margini intorno al grafico
  scale_fill_manual(values = c("Normale" = "#66FF66", "Sospetto" = "dodgerblue3", "Patologico" = "red"))
```
A livello univariato non si evince particolare differenza tra i vari gruppi ad eccezione della variabile *histogram_mean*. Non si esclude che non vi sia corrispondenza a livello bivariato (scatterplot precedenti) o multivariato che non è verificabile con strumenti grafici.  
Si nota nuovamente, a sostegno delle osservazioni precedenti, che il gruppo dei sospetti risulta generalmente più simile al gruppo dei sani. Questo vale per 3 delle 4 variabili.

Inoltre dallo scatterplot delle prime 2 variabili si evince una netta distinzione tra i gruppi così come segue:
<br><br>
```{r, echo=FALSE,fig.align='center', fig.width=5,fig.height=3.5}
fetal_Health %>%
  ggplot(mapping = aes(x=histogram_mean , y = histogram_max, color = fetal_health)) +
  geom_point() +
labs( title = "Clusterizzazione esperti")+
theme(plot.title = element_text(hjust = 0.5,face = "bold", size = 14,margin = margin(10, 0, 20, 0)))+  # hjust Imposta l'allineamento orizzontale al centro
          # margin Imposta i margini intorno al grafico
  scale_color_manual(values = c("Normale" = "#66FF66", "Sospetto" = "dodgerblue3", "Patologico" = "red"))

```
è evidente come i sospetti sul grafico si collocano a destra dei sani e non tra sani e malati, difatti i cardiotogrammi dei feti sospetti sono quelli cui parametri si discostano dalla normalità e che necessitano di ulteriori approfondimenti. 


### 4) Model Based Clustering

Arrivati a questo punto, implementiamo un EM di normali senza specificare il numero dei gruppi:
```{r,echo=FALSE}
#install.packages("mclust")
set.seed(123) 
health_mclust_ICL<-mclustICL(fetal_Health_EM, G=2:10) #l'algoritmo EM non riesce a stimare modelli complessi come VVV a causa della
#bassa disponibilità di u.s. (nemmeno VVV con k=3)
summary(health_mclust_ICL)
```
Essendo un dataset con un numero relativamente limitato di osservazioni l'algoritmo EM non riesce a stimare dei modelli complessi come il VVV(nemmeno VVV con k=3),mentre osserviamo che VEV,7 e EVV,3 sono molto vicini tra loro. Notiamo inoltre dal grafico ottenuto come gran parte dei modelli in base all'ICL risultano della stessa precisione qualunque sia il numero di gruppi. Ipotizziamo che ciò può essere dato dal fatto che essendo dati reali è verosimile che ciascun gruppo abbia diversi sotto gruppi. Alla luce di ciò riteniamo oppurtono eseguire un MDA in fase di model-based classification.

```{r,echo=FALSE,fig.align='center', fig.width=5,fig.height=4}
plot(health_mclust_ICL,ylim=c(-33000,-29000)) 
title("Andamento ICL")
```
Provando a valutare il BIC (non accurato come ICL, in quanto non tiene conto dell'entropia) viene restituito un modello VEV k=8.
```{r,echo=FALSE}
set.seed(123)
health_mclust_BIC<-Mclust(fetal_Health_EM,G=2:10) 
#summary(health_mclust_BIC) # qua niente summary giusto
```
In definitiva possiamo dire che l'EM è molto instabile e poco robusto per cui diventa cruciale la scelta dei valori iniziali. Difatti sia tramite ICL che con il BIC il model based clustering fornisce un numero di gruppi differente. A sto punto proviamo a specificare il numero dei gruppi:
```{r,echo=FALSE}
set.seed(123) 
health_mclust_ICL_k3<-mclustICL(fetal_Health_EM,G=3)
#summary(health_mclust_ICL_k3) #EVV 


set.seed(123)
health_mclust_BIC_k3<-Mclust(fetal_Health_EM,G=3)
#summary(health_mclust_BIC_k3) #EVV
```
In questo caso ICL e BIC restituiscono lo stesso modello. Fossero stati diversi avremmo dato priorità al modello fornito tramite ICL. 
```{r,echo=FALSE}
etichette_stimate<-health_mclust_BIC_k3$classification
precisione_EM<-classError(etichette_stimate, class=etichette)
accuracy<-1-length(precisione_EM$misclassified)/n
```
Grazie alla Confusion Matrix notiamo come la sensitivity è molto alta solo nella classe "Normale",
al contrario la specificity è molto bassa.È evidente come l'EM non riesce a distinguere bene fra il patologico e il normale. Infatti 253 casi sospetti che vengono classificati come "Normali".
```{r,echo=FALSE, warning= FALSE}
etichette_stimate<-as.factor(etichette_stimate)
levels(etichette_stimate)<-c("Patologico","Normale","Sospetto")
prova <- confusionMatrix(etichette_stimate,etichette)
prova$table
```
Successivamente abbiamo plottato il grafico seguente dove in nero rappresentiamo le unità "missclassificate". Si osserva che quanto meno le u.s. facenti parte del gruppo dei malati vengono allocate correttamente.
```{r,echo=FALSE,fig.align='center', fig.width=5,fig.height=4}
grafico1<-coordProj (as.data.frame(fetal_Health_EM), dimens=c(1,2), what="classification",
           classification=health_mclust_BIC_k3$classification,
           col=c("red","dodgerblue3","#66FF66"), symbols=c(0 ,16 ,17),
           sub="(b) Model-Based Clustering") +
  title("Grafico della clusterizzazione EVV")

```

Ai fini di visualizzare meglio l'incertezza ci siamo serviti del grafico che segue. La posizione dei punti ci permette di dedurre dove è stata allocata la corrispondente u.s, mentre l'incertezza della clusterizzazione aumenta al crescere delle dimensioni del punto.

```{r,echo=FALSE,fig.align='center', fig.width=5,fig.height=4}
grafico2<-coordProj (data=as.data.frame(fetal_Health_EM), dimens=c(1,2), what="uncertainty",
           parameters=health_mclust_BIC_k3$parameters , z=health_mclust_BIC_k3$z, ylim=c(130,211),xlim=c(90,165),colors=c("red","dodgerblue3","#66FF66")) + title("Grafico incertezze")

```


```{r , echo = F}
zij<-health_mclust_BIC_k3$z
post_prob<-apply(zij,1,max)
incertezza<-1-post_prob
tibble(index=1:n,incertezza_k3=incertezza)%>%
  arrange(desc(incertezza_k3))%>%
  print(n=10)

zij_best<-Mclust(fetal_Health_EM,G=7,"VEV")$z
post_prob_best<-apply(zij_best,1,max)
incertezza_best<-1-post_prob_best
tibble(index=1:n,incertezza_k7=incertezza_best)%>%
  arrange(desc(incertezza_k7))%>%
  print(n=10)

```






### 5) Model Based Classification
Il primo step dell’analisi è stato quello di dividere il dataset in training e test set. In particolare,abbiamo assegnato, in modo casuale, il 70% delle osservazioni del dataset al training set e il restante 30% al test set. Questa fase è essenziale nel momento in cui si vanno ad applicare approcci
di tipo supervisionato, il cui scopo è quello di fare generalizzazione e prevedere nuove istanze per la
variabile target.
```{r,echo=FALSE}
train_test<-function(data,perc=0.7){
  set.seed(123)
  index<-sample(c("train","test"),size=nrow(data),replace=T,prob=c(perc,1-perc))
  train<-data[index=="train",]
  test<-data[index=="test",]
  lis<-list(data_train=train,
            data_test=test)
  return(lis)
}
#costruzione train e test set per classification:
out<-train_test(fetal_Health_classification,0.7)
data_train<-out$data_train
data_test<-out$data_test
```

#### 5.2) EDDA (Valutato tramite Cross validation) 
Ora che abbiamo ottenuto il traning set alleniamo un modello EDDA: 
```{r,echo=FALSE}
set.seed(123)
pr<-mixmodLearn(data_train[,1:4], c(data_train$fetal_health),
                 models=mixmodGaussianModel(family='all'),
                 criterion=c('CV','BIC'))
#summary(pr) #la parte di evaluation deve essere svolta sul test set
```
Il modello allenato ci restituisce il modello VEV che valutiamo sul test set. Otteniamo la confusion matrix con un valore dell'accuracy dello 0.8447.

```{r,echo=FALSE}
PREDICTION<- mixmodPredict(data = select(data_test,-fetal_health), classificationRule=pr["bestResult"])   
etichette_prediction_EDDA <- as.factor(PREDICTION@partition)
levels(etichette_prediction_EDDA) <- c("Normale","Sospetto", "Patologico")
EDDA_Confusion <- confusionMatrix(etichette_prediction_EDDA,as_factor(pull(data_test,fetal_health)))
EDDA_Confusion$table
```

IL primo è il mio il secondo è di ash.
L'accuracy è elevata solo a causa della differenza di numerosità tra i gruppi; infatti ben 61 dei casi sospetti vengono nuovamente classificati come normali. Notiamo questa "missclassification",seppur in minor parte, anche nei casi patologici (osservabile sia tramite la
confusion matrix che mediante sensitivity e specificity). Plottando il grafico notiamo maggior incertezza nella sezione in cui sono presenti gli individui sospetti.

```{r,echo=FALSE,fig.align='center', fig.width=5,fig.height=3}
prob.post_incertezza<- tibble(PREDICTION@proba) %>%
  rowwise() %>% # operiamo riga per riga
  mutate(incertezza = 1 - max(c_across(everything()))) 
data_test %>%
  ggplot(mapping = aes(x=histogram_mean , y = histogram_max, color = fetal_health)) +
  geom_point(size=prob.post_incertezza$incertezza*10)+
  geom_point(data = filter(data_test,etichette_prediction_EDDA != data_test$fetal_health ), 
             color = "black", alpha = 0.3,size=prob.post_incertezza$incertezza[etichette_prediction_EDDA != data_test$fetal_health]*10)+
labs( title = "Incertezza classificazione EDDA")+
  scale_color_manual(values = c("Normale" = "#66FF66", "Sospetto" = "dodgerblue3", "Patologico" = "red")) +
theme(plot.title = element_text(hjust = 0.5,face = "bold", size = 14,margin = margin(10, 0, 20, 0)))  # hjust Imposta l'allineamento orizzontale al centro
          # margin Imposta i margini intorno al grafico
```

#### 5.3) MDA
Come visto in precedenza sia dal ggpairs che dall'algoritmo EM si evince la necessità di implementare un modello di classificazione di tipo MDA con selezione tramite BIC e successiva valutazione sul test set. Si ottiene un'accuracy dello 0.85.
```{r,echo=FALSE}
set.seed(123)
mod2 <- MclustDA(data_train[,1:4], data_train$fetal_health) 
#summary(mod2)
```

La classe dei sospetti rimane la più problematica stando ai valori di 
sensitivity (che risulta bassa in patologico) e specificity (alta in patologico)
in ogni caso la classificazione tramite MDA risulta migliore di quella eseguita attraverso il modello EDDA.

```{r,echo=FALSE}
# prob.post_incertezza<- tibble(PREDICTION@proba) %>%
#   rowwise() %>% # operiamo riga per riga
#   mutate(incertezza = 1 - max(c_across(everything()))) 
# ```
# 
# ##### 5.3.2)MDA (Cross Validation) 
# ```{r,echo=FALSE}
# accuracy<-function(g,mod,nCV=5,data,etichette){
#   set.seed(123)
#   mod_mda<-MclustDA(data,class=etichette,G=as.list(g),modelName=mod)
#   return(1-cvMclustDA(mod_mda,nfold=nCV)$ce)
# }
# 
# modello_MDA_k3<-function(data,etichette){
#   g1<-g2<-g3<-c(1,2,3,4,5)
#   g1<-as.data.frame(g1)
#   g2<-as.data.frame(g2)
#   g3<-as.data.frame(g3)
#   join<-cross_join(cross_join(g1,g2),g3)
#   join["mod"]<-"VII" #altrimenti con più modelli il codice impegherebbe troppo tempo
#   #usiamo come alternativa il modello VII  che sono delle ipersfere del quale varia solo il volume
#   out<-apply(join,MARGIN=1,function(pos) accuracy(g=pos[1:3],mod=pos[4],nCV=4,data=data,etichette=etichette))
#   lis<-list(modello=join[which.max(out),],accuracy=out[which.max(out)]) #questa accuracy non è valida siccome è stimata sullo stesso dataset usato
#   #per allenaere il modello (fuori dalla funzione viene valutato su un test set)
#   return(lis)
# }
# 
# 
# #(out<-modello_MDA_k3(data_train[,1:4],as.factor(data_train$fetal_health))) #G=(5,4,5)
# #stimiasmo il modello migliore e sul test set forniamo la precisione tramite accuracy e la confusion matrix
# 
# set.seed(123)
# mod_mda_k3<-MclustDA(data_train[,1:4],data_train$fetal_health,G=c(5,4,5),modelNames="VII")
# etichette_prediction_MDA_cv<-predict(mod_mda_k3, select(data_test,-fetal_health))$class
# confusionMatrix(etichette_prediction_MDA_cv, data_test$fetal_health) 

```
Abbiamo implementato una funzione che valuta il miglior modello mda tramite cross validation con nfold uguale a 4 e restituisce il più accurato:
```{r,echo=FALSE}
etichette_prediction_MDA<-predict(mod2, select(data_test,-fetal_health))$class
MDA_Confusion <- confusionMatrix(etichette_prediction_MDA,as_factor(pull(data_test,fetal_health)))
MDA_Confusion$table
```

Il modello trova difficoltà nel distinguere fra "Sospetto" e "Patologico"
abbiamo aumentato la sensitivity del caso "sospetto" ma peggiorato quello del caso "patologico"

#### 5.4) Smote 2.0 
Ci serviamo di SMOTE, un algoritmo di bilanciamento dei dati utilizzato per affrontare
il problema del dataset sbilanciatio. Esso genera sinteticamente nuovi esempi per
le classi minoritarie, identificando i K-nearest neighbors (K-vicini prossimi) e creando combinazioni
lineari tra le osservazioni esistenti. Questo processo migliora la rappresentazione
della classe meno frequente nel dataset, aiutando i modelli di classificazione
esponendoli a più esempi di classi meno rappresentate. 
In generale, l'applicazione di SMOTE contribuisce a una classificazione più accurata delle classi minoritarie in un contesto di dataset sbilanciati.

Nel nostro caso applichiamo l'algoritmo smote per allenare il modello con 600 osservazioni per la classe dei patologici (quella di nostro interesse) 300 per la classe dei "dubbiosi" e facciamo undersampling in modo che il classificatore venga allenato su 300 "Normali". Inoltre utilizziamo il numero di "nearest neighbor" di default che è `k = 5`.

```{r , echo = F}
#install.packages("scutr")
library(scutr)

dt <- as.data.frame(data_train)
data_new_patologici  <-oversample_smote(dt, "Patologico" , cls_col = "fetal_health", m = 600)
data_new_sospetto    <-oversample_smote(dt, "Sospetto" , cls_col = "fetal_health", m = 300)

new_train<-data_train %>%
  filter(fetal_health == "Normale") %>%
  sample_n(size=300) %>%
  rbind(tibble(data_new_patologici),tibble(data_new_sospetto) ) %>%
  as.data.frame()

mm <-mixmodGaussianModel(family = "all",
                         free.proportions = F) #modello in cui i pj non sono stimati siccome vengono imposti pari a circa 0.5 dall'azione
#di oversampling e undersampling    
mod = MclustDA(new_train[,-5],
               new_train$fetal_health ,G=1:5,
               models=mm , verbose = F)


etichette_prediction_oversampling <- predict(mod, select(data_test,-fetal_health))$class

# modsmote <- mixmodLearn(new_train[,-5], new_train$fetal_health ,models=mm,
#                         criterion = "CV")
# 
# PREDICTION<- mixmodPredict(data = select(data_test,-fetal_health), classificationRule=modsmote["bestResult"])
real_labels <- as.factor(data_test$fetal_health)

#etichette_prediction_oversampling<-as.factor(PREDICTION@partition)
#levels(etichette_prediction_oversampling)<-c("Normale","Sospetto","Patologico")

SMOTE_Confusion<-confusionMatrix(etichette_prediction_oversampling,real_labels) 
SMOTE_Confusion$table
```
Vediamo dalla confusion matrix che il numero di casi patologici e dubbiosi classificati male si è ridotto e soprattuto il rateo di patologici classificato come normali è calato drasticamente (espandere anche perchè è meglio ) (da espandere il fatto che è meglio miss classifcare un normale e dire che è patologico , rispetto fare il contrario ). 

```{r,  echo = F}
SMOTE_Confusion$byClass[,1:2]
```
Notiamo un aumento della sensitivity del caso patologico da 0.7 a 0.8 e un aumento della sensitivity del caso sospetto da 0.45 a 0.68, per cui è evidente che siamo più accurati nell'identificare un caso abnormali, altresì marcato è una diminuzione della sensitivity dei casi normali.

```{r,echo=FALSE,fig.align='center', fig.width=5,fig.height=3.5}
prob.post_incertezza<- tibble(predict(mod, select(data_test,-fetal_health))$z) %>%
  rowwise() %>% # operiamo riga per riga
  mutate(incertezza = 1 - max(c_across(everything()))) 

data_test %>%
  ggplot(mapping = aes(x=histogram_mean , y = histogram_max, color = real_labels)) +
  geom_point(size=prob.post_incertezza$incertezza*7)+
  geom_point(data = filter(data_test,etichette_prediction_oversampling != real_labels), 
             color = "black", alpha = 0.3,size=prob.post_incertezza$incertezza[etichette_prediction_oversampling != real_labels]*7) + 
  labs( title = "Classificazione tramite smote")+
  scale_color_manual(values = c("Normale" = "#66FF66", "Sospetto" = "dodgerblue3", "Patologico" = "red"))+
theme(plot.title = element_text(hjust = 0.5,face = "bold", size = 14,margin = margin(10, 0, 20, 0)))  # hjust Imposta l'allineamento orizzontale al centro
          # margin Imposta i margini intorno al grafico
```
Dal grafico vediamo come le unità statistiche patologiche son ben distinguibili anche se distanti dal baricentro della classe contenente i casi patologici. 


### 6) Discussione finale
In definitva si può affermare che la classe dei casi sospetti è molto difficile da definire sia nei modelli di clustering che nei modelli di classification. 
Abbiamo anche osservato come i modelli di tipo MDA sono più accurati dei modelli EDDA probabilmente a causa della presenza di sottogruppi in ciascuna classe. 
Tramite l'utilizzo di SMOTE e undersampling otteniamo un risultato migliore per classi minoritarie, ma risulta ovvio che lo scarso numero di dati utilizzati e l'accuratezza comunque relativamente bassa rendano il nostro classificatore non adatto ad usi medici, ma potrebbe diventarlo tramite metodi di classificazione più sofisticata che possono permettere di altri tipi di dati.