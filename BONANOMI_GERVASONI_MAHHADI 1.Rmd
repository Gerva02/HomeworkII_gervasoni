---
title: <center> **Classificazione della salute del feto** </center>
author: <center> Bonanomi Gervasoni Mahhadi </center>
date: <center> 16 Gennaio 2024 </center>
output: 
  html_document:
    self_contained : TRUE
    toc : TRUE
    toc_float: TRUE
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
```{r warning=FALSE, message= FALSE,echo=FALSE }
library(tidyverse)
library(mclust)
library(Rmixmod)
library(GGally)
library(caret)
```
<br><br>
<center> 
#### *Abstract*
</center>
<div style="text-align: justify;">
La lettura delle cardiotografie CTGs, che sono registrazioni grafiche (tracciati) della frequenza cardiaca fetale durante la gravidanza, è soggetta a interpretazioni differenti quando viene eseguita da diversi operatori sanitari. 
Si tratta di una fase essenziale per valutare la salute del feto durante la gravidanza e il travaglio. Essa infatti influenza le decisioni sull'intervento del medico che, a seconda di come viene valutato il feto, opterà ad esempio per la somministrazione di ossigeno alla madre, o in casi più gravi, procederà con un taglio cesareo d'emergenza.

L'obiettivo di questa nostra analisi sarà costruire dei modelli in grado di prevedere con una buona accuratezza se il feto è Normale, Sospetto o Patologico.
I dati utilizzati sono stati estratti dal database di **SisPorto**, un programma di analisi automatizzata del tracciato cardiotografico che individua e quantifica, seguendo i criteri guida della FIGO (Federazione Internazionale di Ginecologia e Ostetricia), una serie di parametri rilevanti che vengono valutati durante il monitoraggio cardiotografico del feto. Utilizzeremo diverse metodologie di classificazione per valutare e costruire il modello che più si adatta ai nostri dati. A seguito di una breve analisi esplorativa verranno applicate in primis delle tecniche di clustering e successivamente una serie di modelli di classificazione di tipo EDDA o MDA supportate da tecniche di oversampling e undersampling. 
<div style="text-align: justify;">
### 1) Introduzione
La riduzione della mortalità neonatale fa parte dei principi cardine dell'Agenda 2030 per lo sviluppo sostenibile. Sottoscritta nel 2015 dai governi dei 193 membri delle Nazioni Unite, fra gli obiettivi che si prefigge vi è quello di porre fine alle morti prevenibili di neonati e bambini sotto i 5 anni di età. In particolare viene richiesto che tutti i paesi dovranno cercare di ridurre la mortalità neonatale ad almeno 12 per ogni 1000 abitanti nati vivi. Ad oggi però sono più di 60 gli stati lontani da questo traguardo e 2.3 milioni di neonati muoiono ogni anno.
Di questi 904 400 neonati sopravvivono solo pochi giorni a causa di complicanze alla nascita, e oltre un milione sono quelli che muoiono durante il parto. 
La maggior parte di questi decessi sono causati dalla scarsa qualità dell’assistenza prima e durante il parto. Il miglioramento dei servizi sanitari e il potenziamento dei supporti informatici a costi accessibili,quindi, sono punti chiave per arginare queste tragedie.<br><br>
Fatte queste premesse, non si può non considerare la cardiotografia computerizzata CTGc vista la posizione di primaria importanza che occupa nel fornire l'ausilio al monitoraggio della salute del feto. Essa infatti è stata introdotta con l’obiettivo di ridurre i falsi positivi e la soggettività dell’interpretazione poichè fornisce un’interpretazione oggettiva della
CTG consentendo la comunicazione di numeri piuttosto che di opinioni. Negli anni infatti ci si è resi conto che i risultati pratici conseguiti dall’uso «di massa» della cardiotocografia sono stati di gran lunga inferiori alle aspettative a causa della carenza di criteri univoci di interpretazione e a scarsa preparazione degli operatori sanitari.

### 2) Materiali
I dati di interesse sono stati estratti dal dataset **fetal_health**,esportabile gratuitamente dal link citato a fine paragrafo. Si tratta di un dataset con 2126 osservazioni di cardiotografie fetali suddivise per 21 variabili a cui si somma l'etichetta di classe *fetal_health* rappresentata dalle seguenti modalità: 1,2,3 che indicano rispettivamente uno stato di salute del feto Normale, Sospetto, Patologico. Per completezza di informazione segue una tabella che riporta sinteticamente la descrizione e le modalità dell variabili:


VARIABILE     | DESCRIZIONE  | MODALITÀ 
------------- | -------------| ---------
baseline_value| Valore di base della frequenza cardiaca fetale (FHR) |  Bpm
accelerations|Conteggio delle accelerazioni medie dalla FHR | Hz
fetal_movement|Presenza media di movimenti fetali| Hz                
uterine_contractions| Numero di contrazioni uterine| Hz
light_decelerations|Numero di decelerazioni leggere della FHR|Hz            
severe_decelerations|Numero di decelerazioni gravi della FHR|Hz          
prolongued_decelerations|Numero di decelerazioni prolungate della FHR|Hz |    
abnormal_short_term_variability|Presenza di variabilità a breve termine anormale|Binario (0 o 1)|
mean_value_of_short_term_variability| Valore medio della variabilità a breve termine|Numerico|  
percentage_of_time_with_abnormal_long_term...| Percentuale di tempo con variabilità a lungo termine anormale | Percentuale|                      
mean_value_of_long_term_variability| Valore medio della variabilità a lungo termine| Numerico|   
histogram_width| Larghezza dell'istogramma della FHR| Numerico|                       
histogram_min| Valore minimo dell'istogramma della FHR| Numerico|                         
histogram_max| Valore massimo dell'istogramma della FHR| Numerico| 
histogram_number_of_peaks| Numero di picchi nell'istogramma della FHR| Numerico (conteggio)|
histogram_number_of_zeroes| Numero di zeri nell'istogramma della FHR| Numerico (conteggio)|
histogram_mode | Modalità dell'istogramma della FHR | Numerico|
histogram_mean| Valore medio dell'istogramma della FHR| Numerico|              histogram_median| Mediana dell'istogramma della FHR| Numerico |               histogram_variance| Varianza dell'istogramma della FHR| Numerico|
histogram_tendency| Asimmetria dell'istogramma della FHR  | **-1**:sinistra **0**:simmetrica **1**:destra |
fetal_health| Salute fetale (etichetta di classe)|**1**:Sano **2**:Sospetto **3**:Patologico |                     
                 
Il dataset non presenta valori mancanti. Valutando la distribuzione delle variabili abbiamo visto come quelle di conteggio naturalmente seguono una distribuzione assimilabile a quella di Poisson di conseguenza si è optato per escluderle dalle successive analisi assieme alle variabili che dimostravano una pronunciata asimmetria. È stata esclusa anche la variabile histogram_tendency poichè fattoriale. 
Da un'analisi preliminare dell'istogramma delle frequenze relative condizionato per classi si evince come la classe dei sani è predominante:

```{r, echo=FALSE,fig.align='center', fig.width=5,fig.height=3}
fetal_Health <- tibble(read.csv("fetal_health.csv")) %>%
  mutate_at(vars(fetal_health),as.factor) # non è elegante da migliorare
levels(fetal_Health$fetal_health) <- c("Normale","Sospetto","Patologico")
fetal_Health <- fetal_Health %>%
  select(-c(severe_decelerations,
            histogram_number_of_zeroes,
            histogram_number_of_peaks,
            histogram_variance,
            histogram_tendency, 
            percentage_of_time_with_abnormal_long_term_variability, 
            accelerations, 
            fetal_movement,
            prolongued_decelerations))

etichette<-fetal_Health$fetal_health

fetal_Health %>%
ggplot(aes(x=fetal_health,
                y= after_stat(count)/sum(after_stat(count)))) + 
  geom_bar(aes(fill = fetal_health), color="black") +         
  labs(x="Condizione del feto", y="Frequenza Relativa") 
```
Ulteriori dettagli sul dataset sono presenti al seguente link: [Fetal Health Classification-Kaggle](https://www.kaggle.com/datasets/andrewmvd/fetal-health-classification/data>) 

### 3) Analisi esplorativa
In prima analisi abbiamo deciso di analizzare la correlazione mediante ggpairs. Se ne è dedotto che "histogram_mean", "histogram_median" e "histogram_mode" sono fortemente correlate. Si nota come le variabili anche se distinte per fetal_health sono multimodali quindi successivamente ci serviremo della MDA. Inoltre essendo dati presi da macchine, quindi "data entry",è improbabile la presenza di outliers.
```{r, echo=FALSE,fig.align='center', fig.width=5,fig.height=3}

```
Procediamo ora con l'analisi delle componenti principali. Anzitutto, visto l'ordine di grandezza differente fra le variabili abbiamo eseguito una standardizzazione. Con la PCA abbiamo ottenuto che 4 componenti coprono l'80% della variabilità. A questo punto abbiamo selezionato le prime 4 variabili in base ai loadings (cioè a quanto peso ha la singola variabile all'interno della componente principale).
Dopodichè abbiamo fatto un ggpairs delle variabili selezionate. Dal grafico si nota che
le ultime 2 variabili sezionate ("mean_value_of_long_term_variability" e "uterine_contractions")
non rispettano la normalità e la simmetria in tutti i gruppi.
```{r, echo=FALSE}
n <- nrow(fetal_Health)
#analisi delle componenti principali: 
pca <- fetal_Health%>%
  select(-fetal_health)%>%
  princomp(cor=T) 
k <- ncol(fetal_Health)
#selezioniamo le prime 4 variabili in base ai loadings (quanto peso ha la singola 
#variabile all'interno della componente principale)
(load_vars <- names(fetal_Health)[apply(pca$loadings[,1:4], 2, function(x) which(x**2==max(x**2)))])

```
Inoltre dallo scatterplot delle prime 2 variabili si evince una netta distinzione tra i gruppi così come segue:
<br><br>
```{r, echo=FALSE,fig.align='center', fig.width=5,fig.height=4}
fetal_Health %>%
  ggplot(mapping = aes(x=histogram_mean , y = histogram_max, color = fetal_health)) +
  geom_point()

```
è evidente come i sospetti sul grafico si collocano a destra dei sani e non tra sani e malati, difatti i cardiotogrammi dei feti sospetti sono quelli cui parametri si discostano dalla normalità e che necessitano di ulteriori approfondimenti. 


### 4.1) Model Based Clustering
Fatte le dovute analisi, per il clustering abbiamo optato per il dataset con le sole variabili selezionate tramite pca (non servono le etichette per il clustering).
```{r,echo=FALSE,fig.align='center', fig.width=6,fig.height=4}
#dataset per model-based clustering:
fetal_Health_EM<-fetal_Health%>%
  select(all_of(load_vars)) #dataset solo con le variabili selezionate tramite pca (non servono le etichette per il clustering)

#dataset per model-based classification
fetal_Health_classification <-fetal_Health%>%
  select(all_of(load_vars),fetal_health)

fetal_Health_viz <- fetal_Health_classification %>%
  gather(key = "Variable", value = "Value", -fetal_health)

# Create a facetted box plot using ggplot2
ggplot(fetal_Health_viz, aes(x = fetal_health, y = Value, fill =fetal_health )) +
  geom_boxplot() +
  facet_wrap(~Variable, scales = "free_y", ncol = 2) +
  labs(x = "Species",
       y = "Value",
       fill = "Variable") +
  theme_minimal()
```
A livello univariato non si evince particolare differenza tra i vari gruppi ad eccezione della variabile "histogram_mean". Non si esclude che non vi sia corrispondenza a livello bivariato (scatterplot precedenti) o multivariato che non è verificabile con strumenti grafici.  
Si nota nuovamente, a sostegno delle osservazioni precedenti, che il gruppo dei sospetti risulta generalmente più simile al gruppo dei sani. Questo vale per 3 delle 4 variabili.

Arrivati a questo punto, implementiamo un EM di normali senza specificare il numero dei gruppi:
```{r,echo=FALSE}
#install.packages("mclust")
set.seed(123) 
health_mclust_ICL<-mclustICL(fetal_Health_EM, G=2:10) #l'algoritmo EM non riesce a stimare modelli complessi come VVV a causa della
#bassa disponibilità di u.s. (nemmeno VVV con k=3)
summary(health_mclust_ICL)
```
Essendo un dataset con un numero relativamente limitato di osservazioni l'algoritmo EM non riesce a stimare dei modelli complessi come il VVV(nemmeno VVV con k=3),mentre osserviamo che VEV,7 e EVV,3 sono molto vicini tra loro. Notiamo inoltre dal grafico ottenuto come gran parte dei modelli in base all'ICL risultano della stessa precisione qualunque sia il numero di gruppi. Ipotizziamo che ciò può essere dato dal fatto che essendo dati reali è verosimile che ciascun gruppo abbia diversi sotto gruppi. Alla luce di ciò riteniamo oppurtono eseguire un MDA in fase di model-based classification.

```{r,echo=FALSE,fig.align='center', fig.width=5,fig.height=4}
plot(health_mclust_ICL,ylim=c(-33000,-29000)) 

```
Provando a valutare il BIC (non accurato come ICL, in quanto non tiene conto dell'entropia) viene restituito un modello VEV k=8.
```{r,echo=FALSE}
set.seed(123)
health_mclust_BIC<-Mclust(fetal_Health_EM,G=2:10) 
#summary(health_mclust_BIC) # qua niente summary giusto
```
In definitiva possiamo dire che l'EM è molto instabile e poco robusto per cui diventa cruciale la scelta dei valori iniziali). Difatti sia tramite ICL che con il BIC il model based clustering fornisce un numero di gruppi differente. A sto punto proviamo a specificare il numero dei gruppi:
```{r,echo=FALSE}
set.seed(123) 
health_mclust_ICL_k3<-mclustICL(fetal_Health_EM,G=3)
#summary(health_mclust_ICL_k3) #EVV 


set.seed(123)
health_mclust_BIC_k3<-Mclust(fetal_Health_EM,G=3)
#summary(health_mclust_BIC_k3) #EVV
```
In questo caso ICL e BIC restituiscono lo stesso modello. Fossero stati diversi avremmo dato priorità al modello fornito tramite ICL. 
```{r,echo=FALSE}
etichette_stimate<-health_mclust_BIC_k3$classification
precisione_EM<-classError(etichette_stimate, class=etichette)
accuracy<-1-length(precisione_EM$misclassified)/n
```
Grazie alla Confusion Matrix notiamo come la sensitivity è molto alta solo nella classe "Normale",
al contrario la specificity è molto bassa.È evidente come l'EM non riesce a distinguere bene fra il patologico e il normale. Infatti 253 casi sospetti che vengono classificati come "Normali".
```{r,echo=FALSE, warning= FALSE}
etichette_stimate<-as.factor(etichette_stimate)
levels(etichette_stimate)<-c("Patologico","Normale","Sospetto")
prova <- confusionMatrix(etichette_stimate,etichette)
prova$table
```
Successivamente abbiamo plottato il grafico seguente dove in nero rappresentiamo le unità "missclassificate". Si osserva che quanto meno le u.s. facenti parte del gruppo dei malati vengono allocate correttamente.
```{r,echo=FALSE,fig.align='center', fig.width=5,fig.height=4}
coordProj (as.data.frame(fetal_Health_EM), dimens=c(1,2), what="classification",
           classification=health_mclust_BIC_k3$classification,
           col=c("dodgerblue2","green3","red2"), symbols=c(0 ,16 ,17),
           sub="(b) Model-Based Clustering")
points(fetal_Health_EM[precisione_EM$misclassified,c(1,2)],pch=19)
```
Ai fini di visualizzare meglio l'incertezza ci siamo serviti del grafico che segue. Più è grande il pallino nero, maggiore è l'incertezza.
```{r,echo=FALSE,fig.align='center', fig.width=5,fig.height=4}
coordProj (data=as.data.frame(fetal_Health_EM), dimens=c(1,2), what="uncertainty",
           parameters=health_mclust_BIC_k3$parameters , z=health_mclust_BIC_k3$z, ylim=c(130,211),xlim=c(90,165))+
labs( title = "Incertezza clusterizzazione")

```


```{r , echo = F}
(pj<-health_mclust_BIC_k3$parameters$pro)
zij<-health_mclust_BIC_k3$z

(entropia<-(-1)*sum(rowSums(zij*log(zij))))
(entropia_relativa<-entropia/n/log(3))  #non male

mu1<-health_mclust_BIC_k3$parameters$mean[,1]
mu2<-health_mclust_BIC_k3$parameters$mean[,2]
mu3<-health_mclust_BIC_k3$parameters$mean[,3]
(mu<-mu1*pj[1]+mu2*pj[2]+mu3*pj[3])

sigma1<-matrix(health_mclust_BIC_k3$parameters$variance$sigma[1:16],nrow=4,ncol=4,byrow=T)
sigma2<-matrix(health_mclust_BIC_k3$parameters$variance$sigma[17:32],nrow=4,ncol=4,byrow=T)
sigma3<-matrix(health_mclust_BIC_k3$parameters$variance$sigma[33:48],nrow=4,ncol=4,byrow=T)

var_within<-pj[1]*sigma1+pj[2]*sigma2+pj[3]*sigma3
var_between<-pj[1]*(mu1-mu)%*%t(mu1-mu)+pj[2]*(mu2-mu)%*%t(mu2-mu)+pj[3]*(mu3-mu)%*%t(mu3-mu)
(var_mixture<-var_within+var_between)

(R2_tr<-1-sum(diag(var_within))/sum(diag(var_mixture))) #0.27 (male)
(R2_det<-1-det(var_within)/det(var_mixture)) #0.73 (non così male)

post_prob<-apply(zij,1,max)
incertezza<-1-post_prob
(unita_incerte<-tibble(index=1:n,incertezza=incertezza)%>%
  arrange(desc(incertezza))%>%
  print(n=10))

fetal_Health$fetal_health[unita_incerte$index[1:20]] 
```


```{r , echo=FALSE}
KLs<-function(mu1,mu2,sigma1,sigma2) {
  return(0.5*t(mu1-mu2)%*%(solve(sigma1)+solve(sigma2))%*%(mu1-mu2)+0.5*sum(diag(sigma1%*%solve(sigma2)+solve(sigma1)%*%sigma2))-length(mu1)) #gervi dimmi se è giusta...
  }

KLs_matrix<-matrix(0,nrow=3,ncol=3)
KLs_matrix[1,2]<-KLs_matrix[2,1]<-KLs(mu1,mu2,sigma1,sigma2)
KLs_matrix[1,3]<-KLs_matrix[3,1]<-KLs(mu1,mu3,sigma1,sigma3)
KLs_matrix[3,2]<-KLs_matrix[2,3]<-KLs(mu3,mu2,sigma3,sigma2)
KLs_matrix #non  ha nulla a che vedere coi i gruppi reali....
```






### 5.1) Model Based Classification
Il primo step dell’analisi è stato quello di dividere il dataset in training e test set. In particolare,abbiamo assegnato, in modo casuale, il 70% delle osservazioni del dataset al training set e il restante 30% al test set. Questa fase è essenziale nel momento in cui si vanno ad applicare approcci
di tipo supervisionato, il cui scopo è quello di fare generalizzazione e prevedere nuove istanze per la
variabile target.
```{r,echo=FALSE}
train_test<-function(data,perc=0.7){
  set.seed(123)
  index<-sample(c("train","test"),size=nrow(data),replace=T,prob=c(perc,1-perc))
  train<-data[index=="train",]
  test<-data[index=="test",]
  lis<-list(data_train=train,
            data_test=test)
  return(lis)
}
#costruzione train e test set per classification:
out<-train_test(fetal_Health_classification,0.7)
data_train<-out$data_train
data_test<-out$data_test
```

#### 5.2)EDDA (Valutato tramite Cross validation) 
Ora che abbiamo ottenuto il traning set alleniamo un modello EDDA: 
```{r,echo=FALSE}
set.seed(123)
pr<-mixmodLearn(data_train[,1:4], c(data_train$fetal_health),
                 models=mixmodGaussianModel(family='all'),
                 criterion=c('CV','BIC'))
#summary(pr) #la parte di evaluation deve essere svolta sul test set
```
Il modello allenato ci restituisce il modello VEV che valutiamo sul test set. Otteniamo la confusion matrix con un valore dell'accuracy dello 0.8447.

```{r,echo=FALSE}
PREDICTION<- mixmodPredict(data = select(data_test,-fetal_health), classificationRule=pr["bestResult"])   
etichette_prediction_EDDA <- as.factor(PREDICTION@partition)
levels(etichette_prediction_EDDA) <- c("Normale","Sospetto", "Patologico")
EDDA_Confusion <- confusionMatrix(etichette_prediction_EDDA,as_factor(pull(data_test,fetal_health)))
EDDA_Confusion$table
```
<center>
+-------------+-------------+--------------+------------+
|             |             | **PREDICTED**|            | 
+-------------+-------------+--------------+------------+
|**CLASS**    | Normale     | Sospetto     | Patologico | 
+-------------+-------------+--------------+------------+
| Normale     | 494         | 61           | 26         | 
+-------------+-------------+--------------+------------+
| Sospetto    |  1          | 20           | 9          | 
+-------------+-------------+--------------+------------+
| Patologico  | 0           | 1            | 19         | 
+-------------+-------------+--------------+------------+
</center>

IL primo è il mio il secondo è di ash.
L'accuracy è elevata solo a causa della differenza di numerosità tra i gruppi; infatti ben 61 dei casi sospetti vengono nuovamente classificati come normali. Notiamo questa "missclassification",seppur in minor parte, anche nei casi patologici (osservabile sia tramite la
confusion matrix che mediante sensitivity e specificity). Plottando il grafico notiamo maggior incertezza nella sezione in cui sono presenti gli individui sospetti.

```{r,echo=FALSE,fig.align='center', fig.width=5,fig.height=3}
prob.post_incertezza<- tibble(PREDICTION@proba) %>%
  rowwise() %>% # operiamo riga per riga
  mutate(incertezza = 1 - max(c_across(everything()))) 
data_test %>%
  ggplot(mapping = aes(x=histogram_mean , y = histogram_max, color = fetal_health)) +
  geom_point(size=prob.post_incertezza$incertezza*10)+
  geom_point(data = filter(data_test,etichette_prediction_EDDA != data_test$fetal_health ), 
             color = "black", alpha = 0.3,size=prob.post_incertezza$incertezza[etichette_prediction_EDDA != data_test$fetal_health]*10)+
labs( title = "Incertezza classificazione EDDA")

```

#### 5.3.1)MDA
Come visto in precedenza sia dal ggpairs che dall'algoritmo EM si evince la necessità di implementare un modello di classificazione di tipo MDA con selezione tramite BIC e successiva valutazione sul test set. Si ottiene un'accuracy dello 0.85.
```{r,echo=FALSE}
set.seed(123)
mod2 <- MclustDA(data_train[,1:4], data_train$fetal_health) 
#summary(mod2)
```
<center>
da vedere se mettere

+-------------+-------------+--------------+------------+
|             |             | **PREDICTED**|            | 
+-------------+-------------+--------------+------------+
|**CLASS**    | Normale     | Sospetto     | Patologico | 
+-------------+-------------+--------------+------------+
| Normale     | 1088        | 44           | 28         | 
+-------------+-------------+--------------+------------+
| Sospetto    | 104         | 102          | 7          | 
+-------------+-------------+--------------+------------+
| Patologico  | 27          | 14           | 81         | 
+-------------+-------------+--------------+------------+
</center>
La classe dei sospetti rimane la più problematica stando ai valori di 
sensitivity (che risulta bassa in patologico) e specificity (alta in patologico)
in ogni caso la classificazione tramite MDA risulta migliore di quella eseguita attraverso il modello EDDA.

```{r,echo=FALSE}
# prob.post_incertezza<- tibble(PREDICTION@proba) %>%
#   rowwise() %>% # operiamo riga per riga
#   mutate(incertezza = 1 - max(c_across(everything()))) 
# ```
# 
# ##### 5.3.2)MDA (Cross Validation) 
# ```{r,echo=FALSE}
# accuracy<-function(g,mod,nCV=5,data,etichette){
#   set.seed(123)
#   mod_mda<-MclustDA(data,class=etichette,G=as.list(g),modelName=mod)
#   return(1-cvMclustDA(mod_mda,nfold=nCV)$ce)
# }
# 
# modello_MDA_k3<-function(data,etichette){
#   g1<-g2<-g3<-c(1,2,3,4,5)
#   g1<-as.data.frame(g1)
#   g2<-as.data.frame(g2)
#   g3<-as.data.frame(g3)
#   join<-cross_join(cross_join(g1,g2),g3)
#   join["mod"]<-"VII" #altrimenti con più modelli il codice impegherebbe troppo tempo
#   #usiamo come alternativa il modello VII  che sono delle ipersfere del quale varia solo il volume
#   out<-apply(join,MARGIN=1,function(pos) accuracy(g=pos[1:3],mod=pos[4],nCV=4,data=data,etichette=etichette))
#   lis<-list(modello=join[which.max(out),],accuracy=out[which.max(out)]) #questa accuracy non è valida siccome è stimata sullo stesso dataset usato
#   #per allenaere il modello (fuori dalla funzione viene valutato su un test set)
#   return(lis)
# }
# 
# 
# #(out<-modello_MDA_k3(data_train[,1:4],as.factor(data_train$fetal_health))) #G=(5,4,5)
# #stimiasmo il modello migliore e sul test set forniamo la precisione tramite accuracy e la confusion matrix
# 
# set.seed(123)
# mod_mda_k3<-MclustDA(data_train[,1:4],data_train$fetal_health,G=c(5,4,5),modelNames="VII")
# etichette_prediction_MDA_cv<-predict(mod_mda_k3, select(data_test,-fetal_health))$class
# confusionMatrix(etichette_prediction_MDA_cv, data_test$fetal_health) 

```
Abbiamo implementato una funzione che valuta il miglior modello mda tramite cross validation con nfold uguale a 4 e restituisce il più accurato:
```{r,echo=FALSE}
etichette_prediction_MDA<-predict(mod2, select(data_test,-fetal_health))$class
MDA_Confusion <- confusionMatrix(etichette_prediction_MDA,as_factor(pull(data_test,fetal_health)))
MDA_Confusion$table
```

<center>
+-------------+-------------+--------------+------------+
|             |             | **PREDICTED**|            | 
+-------------+-------------+--------------+------------+
|**CLASS**    | Normale     | Sospetto     | Patologico | 
+-------------+-------------+--------------+------------+
| Normale     | 453         | 37           | 12         | 
+-------------+-------------+--------------+------------+
| Sospetto    | 34          | 43           | 8          | 
+-------------+-------------+--------------+------------+
| Patologico  | 8           | 2            | 34         | 
+-------------+-------------+--------------+------------+
</center>
Il modello trova difficoltà nel distinguere fra "Sospetto" e "Patologico"
abbiamo aumentato la sensitivity del caso "sospetto" ma peggiorato quello del caso "patologico"

#### 5.3.2)MDA classificazione sospetti come sani o malati

Implementiamo un MDA con lo scopo di classificare gli individui "sospetti" in una delle altre 2 classi funzione per un modello mda con soli 2 gruppi:

```{r,echo=FALSE}
# modello_MDA_k2<-function(data,etichette){
#   g1<-g2<-c(1,2,3,4,5)
#   g1<-as.data.frame(g1)
#   g2<-as.data.frame(g2)
#   join<-cross_join(g1,g2)
#   join["mod"]<-"VII" #altrimenti con più modelli il codice impegherebbe troppo tempo
#   #usiamo come alternativa il modello VII  che sono delle ipersfere del quale varia solo il volume
#   out<-apply(join,MARGIN=1,function(pos) accuracy(g=pos[1:2],mod=pos[3],nCV=4,data=data,etichette=etichette))
#   lis<-list(modello=join[which.max(out),],accuracy=out[which.max(out)])
#   return(lis)
# }
# 
# 
# (fetal_Health_no_sospetti<-fetal_Health_classification%>%
#   filter(fetal_health!="Sospetto"))
# 
# 
# (etichette_k2<-as.factor(fetal_Health_no_sospetti$fetal_health))
# levels(etichette_k2)<-c("Normale","Normale","Patologico")
# modello_MDA_k2(fetal_Health_no_sospetti[,1:4],etichette_k2)
# set.seed(123)
# mod_mda_k2<-MclustDA(fetal_Health_no_sospetti[,1:4],etichette_k2,G=4,modelNames="VII") #G=(4,4)
# summary(mod_mda_k2)
```



```{r,echo=FALSE}
# (fetal_Healt_sospetti<-fetal_Health_classification%>%
#   filter(fetal_health=="Sospetto"))
# 
# table(predict(mod_mda_k2,fetal_Healt_sospetti[,1:4])$class)/nrow(fetal_Healt_sospetti)
```
Stando ai dati si evince che nella gran parte dei casi "sospetti" apparterrebbero alla classe normale

#### Smote 2.0 
Ci serviamo di SMOTE, un algoritmo di bilanciamento dei dati utilizzato per affrontare
il problema dei dataset sbilanciati. Esso genera sinteticamente nuovi esempi per
la classe minoritaria, identificando vicini prossimi e creando combinazioni
lineari tra le istanze esistenti. Questo processo migliora la rappresentazione
della classe meno frequente nel dataset, aiutando i modelli di machine learning a
generalizzare meglio durante l'addestramento. La corretta regolazione dei parametri,
come il numero di vicini, è essenziale per evitare eccessiva generazione di dati sintetici. In generale, l'applicazione di SMOTE contribuisce a una classificazione più accurata delle classi minoritarie in un contesto di dataset sbilanciati.
Nel nostro caso in particolare generiamo 7 dati sintetici per ogni reale e teniamo il numero di k di default.


```{r , echo = F}
#install.packages("scutr")
library(scutr)

dt <- as.data.frame(data_train)
table(data_train$fetal_health)
data_new_patologici  <-oversample_smote(dt, "Patologico" , cls_col = "fetal_health", m = 600)
data_new_sospetto    <-oversample_smote(dt, "Sospetto" , cls_col = "fetal_health", m = 600)

new_train<-data_train %>%
  filter(fetal_health == "Normale") %>%
  rbind(tibble(data_new_patologici),tibble(data_new_sospetto) ) %>%
  as.data.frame()

mm <-mixmodGaussianModel(family = "all",
                         free.proportions = F) #modello in cui i pj non sono stimati siccome vengono imposti pari a circa 0.5 dall'azione
#di oversampling e undersampling                                      
modsmote <- mixmodLearn(new_train[,-5], new_train$fetal_health ,models=mm,
                        criterion = "CV")



PREDICTION<- mixmodPredict(data = select(data_test,-fetal_health), classificationRule=modsmote["bestResult"])

real_labels <- as.factor(data_test$fetal_health)

etichette_prediction_oversampling<-as.factor(PREDICTION@partition)
levels(etichette_prediction_oversampling)<-c("Normale","Sospetto","Patologico")

SMOTE_confusion<-confusionMatrix(etichette_prediction_oversampling,real_labels) 
SMOTE_confusion$table
```

```{r,  echo = F}
SMOTE_confusion$byClass[,1:2]

```


Notiamo un aumento della sensitivity del caso patologico(che corrisponde alla specificity del caso normale) da 0.7 a 0.83, per cui è evidente che siamo più accurati nell'identificare un caso patologico.
Supponendo H0 caso patologico, l'errore di primo tipo che consiste nel classificare un caso patologico come sano è diminuito notevolmente a discapito dell'errore di secondo tipo (classificare un caso sano come patologico).
non dice tutta la storia perchè solo 3/51 vengono miss classificati di malati vengono miss classificati per sani


```{r,echo=FALSE,fig.align='center', fig.width=5,fig.height=3.5}
prob.post_incertezza<- tibble(PREDICTION@proba) %>%
  rowwise() %>% # operiamo riga per riga
  mutate(incertezza = 1 - max(c_across(everything()))) 

data_test %>%
  ggplot(mapping = aes(x=histogram_mean , y = histogram_max, color = real_labels)) +
  geom_point(size=prob.post_incertezza$incertezza*4)+
  geom_point(data = filter(data_test,etichette_prediction_oversampling != real_labels), 
             color = "black", alpha = 0.3,size=prob.post_incertezza$incertezza[etichette_prediction_oversampling != real_labels]*4) + 
  labs( title = "Classificazione tramite smote")
```
Dal grafico vediamo come le unità statistiche patologiche son ben distinguibili anche se distanti dal baricentro della classe contenente i casi patologici. 


### 6) Discussione finale
In definitva si può affermare che la classe dei casi sospetti è molto difficile da definire sia nei modelli di clustering che nei modelli di classification. 
Abbiamo anche osservato come i modelli di tipo MDA sono più accurati dei modelli EDDA probabilmente a causa della presenza di sottogruppi in ciascuna classe. 
Restringendo l'analisi alla sola identificazione dei casi patologici, la classificazione di quest'ultimi risulta molto più precisa e accurata anche grazie alle tecniche di oversampling e undersampling di cui ci siamo serviti per bilanciare le numerosità all'interno delle classi.


