---
title: <center> **Classificazione della salute del feto** </center>
author: <center> Bonanomi Gervasoni Mahhadi </center>
date: <center> 26 Gennaio 2024 </center>
output: 
  html_document:
    self_contained : TRUE
    toc : TRUE
    toc_float: TRUE
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = F)
knitr::opts_chunk$set(comment = NA)

```
```{r warning=FALSE, message= FALSE }
library(tidyverse)
library(mclust)
library(Rmixmod)
library(GGally)
library(caret)
```
<br><br>
<center> 
#### *Abstract*
</center>
<div style="text-align: justify;">
La lettura delle cardiotografie CTGs, che sono registrazioni grafiche (tracciati) della frequenza cardiaca fetale durante la gravidanza, è soggetta a interpretazioni differenti quando viene eseguita da diversi operatori sanitari. 
Si tratta di una fase essenziale per valutare la salute del feto durante la gravidanza e il travaglio. Essa infatti influenza le decisioni sull'intervento del medico che, a seconda di come viene valutato il feto, opterà ad esempio per la somministrazione di ossigeno alla madre, o in casi più gravi, procederà con un taglio cesareo d'emergenza.

L'obiettivo di questa nostra analisi sarà costruire dei modelli in grado di prevedere con una buona accuratezza se il feto è Normale, Sospetto o Patologico.
Utilizzeremo diverse metodologie di classificazione per valutare e costruire il modello che più si adatta ai nostri dati. A seguito di una breve analisi esplorativa verranno applicate in primis delle tecniche di clustering e successivamente una serie di modelli di classificazione di tipo EDDA o MDA supportate da tecniche di oversampling e undersampling. 
<div style="text-align: justify;">
### 1) Introduzione
La riduzione della mortalità neonatale fa parte dei principi cardine dell'Agenda 2030 per lo sviluppo sostenibile. Sottoscritta nel 2015 dai governi dei 193 membri delle Nazioni Unite, fra gli obiettivi che si prefigge vi è quello di porre fine alle morti prevenibili di neonati e bambini sotto i 5 anni di età. In particolare viene richiesto che tutti i paesi dovranno cercare di ridurre la mortalità neonatale ad almeno 12 per ogni 1000 abitanti nati vivi. Ad oggi però sono più di 60 gli stati lontani da questo traguardo e 2.3 milioni di neonati muoiono ogni anno.
Di questi 904 400 neonati sopravvivono solo pochi giorni a causa di complicanze alla nascita, e oltre un milione sono quelli che muoiono durante il parto. 
La maggior parte di questi decessi sono causati dalla scarsa qualità dell’assistenza prima e durante il parto. Il miglioramento dei servizi sanitari e il potenziamento dei supporti informatici a costi accessibili, quindi, sono punti chiave per arginare queste tragedie.<br><br>
Fatte queste premesse, non si può non considerare la cardiotografia computerizzata CTGc vista la posizione di primaria importanza che occupa nel fornire un ausilio al monitoraggio della salute del feto. Essa infatti è stata introdotta con l’obiettivo di ridurre i falsi positivi e la soggettività dell’interpretazione poichè fornisce un’interpretazione oggettiva della CTG consentendo la comunicazione di numeri piuttosto che di opinioni. Negli anni infatti ci si è resi conto che i risultati pratici conseguiti dall’uso «di massa» della cardiotocografia sono stati di gran lunga inferiori alle aspettative a causa della carenza di criteri univoci di interpretazione e alla scarsa preparazione degli operatori sanitari. <br><br>
Grazie alla cardiotografia computerizzata, invece, non solo si riesce ad ottenere una migliore interpretazione del cardiotogramma, ma sfruttando la grande mole di dati raccolti, si è diffusa la sperimentazione di numerosi algoritmi di machine learning. Chiaramente l'introduzione di queste nuove tecniche fornisce risultati utili da valutare congiuntamente all'intero quadro clinico, senza quindi prestese di esaustività.
Nel progetto seguente ci siamo cimentati nello sviluppare alcuni modelli coi quali identificare con maggior precisione quali sono i parametri che più di altri caratterizzano la salute del feto oltrechè prevedere con minore margine di errore i casi che richiedono l'intervento ostetrico. <br>

### 2) Materiali
I dati di interesse provengono dal dataset **fetal_health**, esportabile gratuitamente dal link citato a fine paragrafo. La raccolta di questi dati è stata fatta mediante SisPorto, un programma di analisi automatizzata del tracciato cardiotografico che registra e riporta graficamente, l'attività elettrica del cuore e delle contrazioni uterine. Su queste calcola poi una serie di metriche che vengono valutate durante il monitoraggio cardiotografico del feto, così come indicato dalla FIGO (Federazione Internazionale di Ginecologia e Ostetricia).<br>
Tramite queste rilevazioni si è ottenuto un dataset con 2126 osservazioni di cardiotografie fetali suddivise per 21 variabili a cui si somma l'etichetta di classe *fetal_health* rappresentata dalle modalità Normale, Sospetto, Patologico.
Nel contesto valutativo della cardiotografia un tracciato è definito:

<span style="color:#66FF66;">**Normale**</span> se la frequenza cardiaca fetale e la variabilità, così come le contrazioni uterine, rientrano negli intervalli attesi. Ciò suggerisce che il feto è in buona salute e sta ricevendo un adeguato apporto di ossigeno.<br>
<span style="color:blue;">**Sospetto**</span> quando i valori ottenuti mostrano deviazioni lievi dalla norma, senza implicare necessariamente un problema grave. Potrebbe segnalare la necessità di ulteriori monitoraggi o valutazioni per garantire il benessere del feto. Questa categoria potrebbe includere pattern non ottimali ma che non suggeriscono immediatamente una condizione patologica.<br>
<span style="color:red;">**Patologico**</span> se presenti anomalie che possono rappresentare un rischio per la salute del feto. Ciò potrebbe includere pattern come decelerazioni prolungate, ridotta variabilità o altre irregolarità sintomo di una possibile sofferenza o compromessa ossigenazione del feto. In tali casi, potrebbe essere necessaria un'azione medica e ulteriori valutazioni per affrontare le problematiche sottostanti.

Per completezza di informazione segue una tabella che riporta sinteticamente la descrizione e le modalità dell variabili:


VARIABILE     | DESCRIZIONE  | MODALITÀ 
------------- | -------------| ---------
baseline_value| Frequenza cardiaca fetale di base (FHR) | Valore in Bpm
accelerations|Numero di accelerazioni cardiache al secondo | Valore in Hz |
fetal_movement|Numero di accelerazioni fetali al secondo| Valore in Hz  |              
uterine_contractions| Numero di contrazioni uterine al secondo| Valore in Hz |
light_decelerations|Numero di decelerazioni leggere al secondo| Valore in Hz |         severe_decelerations|Numero di decelerazioni gravi al secondo| Valore in Hz |         
prolongued_decelerations|Numero di decelerazioni prolungate al secondo|Valore in Hz | abnormal_short_term_variability|Percentuale di tempo con variabilità a breve termine anomala|Valore in Percentuale|
mean_value_of_short_term_variability| Media della variabilità a breve termine|Valore in Millisecondi|  
percentage_of_time_with_abnormal_long_term...| Percentuale di tempo con variabilità a lungo termine anomala |Valore in Percentuale|                      
mean_value_of_long_term_variability| Media della variabilità a lungo termine| Valore in Millisecondi|   
histogram_width| Larghezza dell'istogramma della FHR| Valore in Bpm|                       
histogram_min| Valore minimo dell'istogramma della FHR| Valore in Bpm|                      
histogram_max| Valore massimo dell'istogramma della FHR|Valore in Bpm| 
histogram_number_of_peaks| Numero di picchi nell'istogramma della FHR| Valore numerico (conteggio)|
histogram_number_of_zeroes| Numero di zeri nell'istogramma della FHR| Valore numerico (conteggio)|
histogram_mode | Moda dell'istogramma della FHR |Valore in Bpm|
histogram_mean| Valore medio dell'istogramma della FHR| Valore in Bpm|             
histogram_median| Mediana dell'istogramma della FHR| Valore in Bpm |               
histogram_variance| Varianza dell'istogramma della FHR| Valore in Bpm^2|
histogram_tendency| Asimmetria dell'istogramma della FHR  | **-1**:sinistra **0**:simmetrica **1**:destra |
fetal_health| Salute del feto (etichetta di classe)|**1**:Sano **2**:Sospetto **3**:Patologico |                     
Il dataset non presenta valori mancanti. Valutando la distribuzione delle variabili abbiamo visto come quelle di conteggio naturalmente seguono una distribuzione assimilabile a quella di Poisson di conseguenza si è optato per escluderle dalle successive analisi. È stata esclusa anche la variabile *histogram_tendency* poichè fattoriale. <br>
Le variabili *percentage_of_time_with_abnormal_long_term_variability* e *histogram_variance* non son state considerate perchè hanno un comportamento che si discosta fortemente da una distribuzione normale. Le restanti variabili escluse dimostravano una pronunciata asimmetria.
Da un'analisi preliminare dell'istogramma delle frequenze relative condizionato per classi si evince come la classe dei sani è predominante, il che è fisiologico considerando che nella popolazione, l'incidenza di casi patologici è molto limitata:

```{r,fig.align='center', fig.width=5,fig.height=3.5}
fetal_Health <- tibble(read.csv("fetal_health.csv")) %>%
  mutate_at(vars(fetal_health),as.factor) # non è elegante da migliorare
levels(fetal_Health$fetal_health) <- c("Normale","Sospetto","Patologico")
fetal_Health <- fetal_Health %>%
  select(-c(severe_decelerations,
            histogram_number_of_zeroes,
            histogram_number_of_peaks,
            histogram_variance,
            histogram_tendency, 
            percentage_of_time_with_abnormal_long_term_variability, 
            accelerations, 
            fetal_movement,
            prolongued_decelerations))

etichette<-fetal_Health$fetal_health

fetal_Health %>%
ggplot(aes(x=fetal_health,
                y= after_stat(count)/sum(after_stat(count)))) + 
  geom_bar(aes(fill = fetal_health), color="black") +         
  labs(title = "Istogramma frequenza relativa delle classi",x="Condizione del feto", y="Frequenza Relativa")+
theme(plot.title = element_text(hjust = 0.5,face = "bold", size = 14,margin = margin(10, 0, 20, 0)))+  # hjust Imposta l'allineamento orizzontale al centro
          # margin Imposta i margini intorno al grafico
  scale_fill_manual(values = c("Normale" = "#66FF66", "Sospetto" = "dodgerblue3", "Patologico" = "red"))
```
Ulteriori dettagli sul dataset sono presenti al seguente link: [Fetal Health Classification-Kaggle](https://www.kaggle.com/datasets/andrewmvd/fetal-health-classification/data>) 

### 3) Analisi esplorativa
In prima analisi abbiamo deciso di analizzare la correlazione e la distribuzione delle variabili mediante ggpairs. Se ne è dedotto che le variabili *histogram_mean*, *histogram_median* e *histogram_mode* sono fortemente correlate; di conseguenza non è necessario includerne più di una. Non si riesco a dedurre osservazioni particolarmente utili per la fase di clustering a livello univariato o bivariato ma ciò non esclude la presenza di gruppi a livello multivariato (non visualizzabili).Infine si nota che le variabili se distinte per fetal_health sono multimodali quindi successivamente ci serviremo della classificazione MDA. Inoltre essendo dati presi da macchine, è improbabile la presenza di errori dovuti a "data entry" (gli outliers sono altra roba....).

Procediamo ora con l'analisi delle componenti principali. Anzitutto, visto l'ordine di grandezza differente fra le variabili abbiamo eseguito una standardizzazione. Con la PCA abbiamo ottenuto che 4 componenti coprono l'80% della variabilità. A questo punto abbiamo selezionato le prime 4 variabili in base ai loadings (cioè a quanto peso ha la singola variabile all'interno della componente principale).
```{r }
n <- nrow(fetal_Health)
#analisi delle componenti principali: 
pca <- fetal_Health%>%
  select(-fetal_health)%>%
  princomp(cor=T) 
k <- ncol(fetal_Health)
#selezioniamo le prime 4 variabili in base ai loadings (quanto peso ha la singola 
#variabile all'interno della componente principale)
(load_vars <- names(fetal_Health)[apply(pca$loadings[,1:4], 2, function(x) which(x**2==max(x**2)))])

```
Dopodichè abbiamo ripetuto il ggpairs, ma delle solo variabili selezionate. Notiamo che le ultime 2 variabili selezionate *mean_value_of_long_term_variability* e *uterine_contractions* non rispettano la normalità e la simmetria in tutti i gruppi (analizzando dati reali sono situazioni molto frequenti che quindi accettiamo nel modello).
Inoltre dallo scatterplot delle prime 2 variabili si evince una netta distinzione tra i gruppi coi casi sospetti che si collocano a destra dei sani e non al centro,confermando l'anomalia delle osservazioni della seconda classe,sulle cui oservazioni sono richieste ulteriori analisi:
```{r, ,fig.align='center', fig.width=5,fig.height=3.5}
fetal_Health %>%
  ggplot(mapping = aes(x=histogram_mean , y = histogram_max, color = fetal_health)) +
  geom_point(alpha=0.4) +
labs( title = "Scatterplot condizionato")+
theme(plot.title = element_text(hjust = 0.5,face = "bold", size = 14,margin = margin(10, 0, 20, 0)))+  # hjust Imposta l'allineamento orizzontale al centro
          # margin Imposta i margini intorno al grafico
  scale_color_manual(values = c("Normale" = "#66FF66", "Sospetto" = "dodgerblue3", "Patologico" = "red"))

```
Fatte queste premesse, per il clustering abbiamo optato per il dataset con le sole variabili selezionate tramite pca così da avere una buona sintesi delle variabili a costo di perdere interpretabilità nei risultati. Segue la proiezione di un facetted box plot col quale rileviamo alcuni outlier che verranno conservati per mantenere la multimodalità:
```{r,fig.align='center', fig.width=6.5,fig.height=4}
#dataset per model-based clustering:
fetal_Health_EM<-fetal_Health%>%
  select(all_of(load_vars)) #dataset solo con le variabili selezionate tramite pca (non servono le etichette per il clustering)

#dataset per model-based classification
fetal_Health_classification <-fetal_Health%>%
  select(all_of(load_vars),fetal_health)

fetal_Health_viz <- fetal_Health_classification %>%
  gather(key = "Variable", value = "Value", -fetal_health)

# Create a facetted box plot using ggplot2
ggplot(fetal_Health_viz, aes(x = fetal_health, y = Value, fill =fetal_health )) +
  geom_boxplot() +
  facet_wrap(~Variable, scales = "free_y", ncol = 2) +
  labs(x = "Species",
       y = "Value",
       fill = "Variable") +
  theme_minimal() +
labs( title = "BoxPlot componenti principali")+
theme(plot.title = element_text(hjust = 0.5,face = "bold", size = 14,margin = margin(10, 0, 20, 0)))+  # hjust Imposta l'allineamento orizzontale al centro
          # margin Imposta i margini intorno al grafico
  scale_fill_manual(values = c("Normale" = "#66FF66", "Sospetto" = "dodgerblue3", "Patologico" = "red"))
```
A livello univariato non si evince particolare differenza tra i vari gruppi ad eccezione della variabile *histogram_mean* che però potrebbe avere un comportamento simile alle altre a livello bivariato (scatterplot precedenti) o multivariato (non verificabile con strumenti grafici).
Si nota nuovamente, a sostegno delle osservazioni precedenti, che il gruppo dei sospetti risulta generalmente più simile al gruppo dei sani. Questo vale per 3 delle 4 variabili. (boxplot suddivisi per etichette ci danno info solo x classification-----tutto questo da rivedere siccome abbiamo qualche ripetizione di robe già dette prima)


### 4) Model Based Clustering

Arrivati a questo punto, implementiamo un EM di normali senza specificare il numero dei gruppi:
```{r}
#install.packages("mclust")
set.seed(123) 
health_mclust_ICL<-mclustICL(fetal_Health_EM, G=2:10) #l'algoritmo EM non riesce a stimare modelli complessi come VVV a causa della
#bassa disponibilità di u.s. (nemmeno VVV con k=3)
summary(health_mclust_ICL)
```
Essendo un dataset con un numero relativamente limitato di osservazioni l'algoritmo EM non riesce a stimare dei modelli complessi come il VVV (nemmeno VVV con k=3), mentre osserviamo che VEV,7 e EVV,3 sono molto vicini tra loro. Non viene prediletto un k particolare. Inoltre, notiamo dal grafico ottenuto come gran parte dei modelli valutati in base all'ICL risultano della stessa precisione qualunque sia il numero di gruppi. Ipotizziamo che ciò può essere dato dal fatto che essendo dati reali è verosimile che ciascun gruppo abbia diversi sotto gruppi. Alla luce di questo riconfermiamo la nostra intenzione di allenare un MDA in fase di model-based classification.
```{r,,fig.align='center', fig.width=5,fig.height=4}
plot(health_mclust_ICL,ylim=c(-33000,-29000)) 
title("Andamento ICL")
```
Provando a valutare il BIC (non accurato come ICL, in quanto non tiene conto dell'entropia) viene restituito un modello VEV k=8.
```{r}
set.seed(123)
health_mclust_BIC<-Mclust(fetal_Health_EM,G=2:10) 
#summary(health_mclust_BIC) # qua niente summary giusto
```
In definitiva possiamo dire che l'EM è molto instabile e poco robusto per cui diventa cruciale la scelta dei valori iniziali. Difatti sia tramite ICL che con il BIC il model based clustering fornisce un numero di gruppi differente. Provando, invece, a specificare il numero dei gruppi ICL e BIC restituiscono lo stesso modello. Fossero stati diversi avremmo dato priorità al modello fornito tramite ICL per le sue proprietà. 
```{r}
set.seed(123) 
health_mclust_ICL_k3<-mclustICL(fetal_Health_EM,G=3)
#summary(health_mclust_ICL_k3) #EVV 


set.seed(123)
health_mclust_BIC_k3<-Mclust(fetal_Health_EM,G=3)
#summary(health_mclust_BIC_k3) #EVV
```
```{r}
etichette_stimate<-health_mclust_BIC_k3$classification
precisione_EM<-classError(etichette_stimate, class=etichette)
accuracy<-1-length(precisione_EM$misclassified)/n
```

Per avere un riscontro visivo rappresentiamo il grafico della clusterizzazione ottenuta specificando il numero esatto dei gruppi:
```{r,fig.align='center', fig.width=5,fig.height=4}
grafico1<-coordProj (as.data.frame(fetal_Health_EM), dimens=c(1,2), what="classification",
           classification=health_mclust_BIC_k3$classification,
           col=c("red","dodgerblue3","#66FF66"), symbols=c(0 ,16 ,17)) +
  title("Grafico della clusterizzazione EVV")

```

Ai fini di visualizzare meglio l'incertezza,invece, ci siamo serviti del grafico che segue. La posizione dei punti ci permette di dedurre dove è stata allocata la corrispondente u.s., mentre l'incertezza della clusterizzazione aumenta al crescere delle dimensioni del punto:
```{r,fig.align='center', fig.width=5,fig.height=4}
grafico2<-coordProj (data=as.data.frame(fetal_Health_EM), dimens=c(1,2), what="uncertainty",
           parameters=health_mclust_BIC_k3$parameters , z=health_mclust_BIC_k3$z, ylim=c(130,211),xlim=c(90,165),colors=c("red","dodgerblue3","#66FF66")) + title("Grafico incertezze")

```

#### 4.2) Discussione dei risultati del clustering
Sia dal grafico che dalle medie stimate dei singoli gruppi si deduce chiaramente come l'EM non sia in grado di riconoscere correttamente i gruppi.
In particolare, il gruppo dei sospetti viene completamente incluso all'interno del gruppo dei normali e viene creato un nuovo gruppo a noi sconosciuto che si colloca tra il gruppo dei normali e quello dei patologici. Oltretutto esso risulta l'unico gruppo evidenziato correttamente al netto di poche u.s. che anche in origine erano molto distanti dalla gran parte dei soggetti patologici (baricentro).
Queste conclusioni non sono accurate siccome la visualizzazione permette di includere solo 2 variabili mentre il nostro modello si basa su 4 di esse
A seguito di queste osservazioni non è possibile effettuare dei confronti tramite una confusion matrix o calcolare degli indicatori come CER, adjusted rand index o accuracy (a parte sensitivity dei patologici: 0.78) avendo trovato una clusterizzazione totalmente sbagliata. Per questo motivo è solo possibile fornire degli indicatori di bontà del nostro modello che comunque rispecchiano la bassa precisione delle analisi come R2det=0.73 e R2tr=0.27.
Al contrario per quanto riguarda entropia (356) ed entropia relativa (0.15) il modello (avendo pochi gruppi) non risulta inaffidabile.
Interessante è svolgere un confronto con il modello migliore stimato tramite ICL che risulta essere un VEV con 7 gruppi (ad indicare nuovamente che all'interno delle etichette sono presenti dei sotto gruppi) ed è valutato positivamente a livello di R2 (0.89 per quello calcolato con determinante e 0.39 per quello calcolato tramite traccia) ma peggiora di molto in termini di incertezza (molto elevata) nell'allocazione delle u.s. ed entropia (1019 e 0.25 quella relativa).
VEV,7 e EVV,3 sono gli unici gruppi che vale la pena confrontare siccome sono molto più vicini tra loro stando all'ICL rispetto al terzo modello.
DITEMI SE VOLETE FARLO A MO DI TABELLA COI VALORI DEGLI INDICI O SE VOLETE METTERE LE INCERTEZZE DELLE PRIME 10 U.S. MAGGIORMENTE INCERTE

### 5) Model Based Classification
Il primo step dell’analisi è stato quello di dividere il dataset in training e test set. In particolare,abbiamo assegnato, in modo casuale, il 70% delle osservazioni del dataset al training set e il restante 30% al test set. Questa fase è essenziale nel momento in cui si vanno ad applicare approcci
di tipo supervisionato, il cui scopo è quello di fare generalizzazione e prevedere nuove istanze per la variabile target.
```{r}
train_test<-function(data,perc=0.7){
  set.seed(123)
  index<-sample(c("train","test"),size=nrow(data),replace=T,prob=c(perc,1-perc))
  train<-data[index=="train",]
  test<-data[index=="test",]
  lis<-list(data_train=train,
            data_test=test)
  return(lis)
}
#costruzione train e test set per classification:
out<-train_test(fetal_Health_classification,0.7)
data_train<-out$data_train
data_test<-out$data_test
```

#### 5.2) EDDA (Valutato tramite Cross validation) 
Ora che abbiamo ottenuto il traning set alleniamo un modello EDDA.Il modello allenato ci restituisce il modello VEV che valutiamo sul test set. Otteniamo la confusion matrix con un valore dell'accuracy dello 0.8447.
```{r,}
set.seed(123)
pr<-mixmodLearn(data_train[,1:4], c(data_train$fetal_health),
                 models=mixmodGaussianModel(family='all'),
                 criterion=c('CV','BIC'))
#summary(pr) #la parte di evaluation deve essere svolta sul test set
```
```{r}
PREDICTION<- mixmodPredict(data = select(data_test,-fetal_health), classificationRule=pr["bestResult"])   
etichette_prediction_EDDA <- as.factor(PREDICTION@partition)
levels(etichette_prediction_EDDA) <- c("Normale","Sospetto", "Patologico")
EDDA_Confusion <- confusionMatrix(etichette_prediction_EDDA,as_factor(pull(data_test,fetal_health)))
EDDA_Confusion$table
```

L'accuracy è elevata solo a causa della differenza di numerosità tra i gruppi. Notiamo questa mal classificazione sia nei casi sospetti sia,seppur in minor parte, nei casi patologici. Si può osservare tramite la
confusion matrix o mediante le sensitivity molto basse (0.24 e 0.35 di sospetti e patologici). 
Plottando il grafico notiamo maggior incertezza nella regione in cui sono presenti gli individui sospetti ed in parte anche negli individui patologici:
```{r,fig.align='center', fig.width=5,fig.height=3}
prob.post_incertezza<- tibble(PREDICTION@proba) %>%
  rowwise() %>% # operiamo riga per riga
  mutate(incertezza = 1 - max(c_across(everything()))) 
data_test %>%
  ggplot(mapping = aes(x=histogram_mean , y = histogram_max, color = fetal_health)) +
  geom_point(size=prob.post_incertezza$incertezza*10)+
  geom_point(data = filter(data_test,etichette_prediction_EDDA != data_test$fetal_health ), 
             color = "black", alpha = 0.3,size=prob.post_incertezza$incertezza[etichette_prediction_EDDA != data_test$fetal_health]*10)+
labs( title = "Incertezza classificazione EDDA")+
  scale_color_manual(values = c("Normale" = "#66FF66", "Sospetto" = "dodgerblue3", "Patologico" = "red")) +
theme(plot.title = element_text(hjust = 0.5,face = "bold", size = 14,margin = margin(10, 0, 20, 0)))  # hjust Imposta l'allineamento orizzontale al centro
          # margin Imposta i margini intorno al grafico
```

#### 5.3) MDA
Come preannunciato, considerato il ggpairs e l'algoritmo EM ottenuti, si evince la necessità di implementare un modello di classificazione di tipo MDA con selezione tramite BIC e successiva valutazione sul test set.
```{r}
set.seed(123)
mod2 <- MclustDA(data_train[,1:4], data_train$fetal_health) 
#summary(mod2)
```

La classe dei sospetti resta la più problematica stando ai valori della sensitivity ma ha subito comunque un discreto incremento rispetto alla classificazione EDDA (si può affermare che è meglio un modello MDA)
Proviamo a valuatre un mda basato su cross validation:
```{r}
# prob.post_incertezza<- tibble(PREDICTION@proba) %>%
#   rowwise() %>% # operiamo riga per riga
#   mutate(incertezza = 1 - max(c_across(everything()))) 
# ```
# 
#
# accuracy<-function(g,mod,nCV=5,data,etichette){
#   set.seed(123)
#   mod_mda<-MclustDA(data,class=etichette,G=as.list(g),modelName=mod)
#   return(1-cvMclustDA(mod_mda,nfold=nCV)$ce)
# }
# 
# modello_MDA_k3<-function(data,etichette){
#   g1<-g2<-g3<-c(1,2,3,4,5)
#   g1<-as.data.frame(g1)
#   g2<-as.data.frame(g2)
#   g3<-as.data.frame(g3)
#   join<-cross_join(cross_join(g1,g2),g3)
#   join["mod"]<-"VII" #altrimenti con più modelli il codice impegherebbe troppo tempo
#   #usiamo come alternativa il modello VII  che sono delle ipersfere del quale varia solo il volume
#   out<-apply(join,MARGIN=1,function(pos) accuracy(g=pos[1:3],mod=pos[4],nCV=4,data=data,etichette=etichette))
#   lis<-list(modello=join[which.max(out),],accuracy=out[which.max(out)]) #questa accuracy non è valida siccome è stimata sullo stesso dataset usato
#   #per allenaere il modello (fuori dalla funzione viene valutato su un test set)
#   return(lis)
# }
# 
# 
# #(out<-modello_MDA_k3(data_train[,1:4],as.factor(data_train$fetal_health))) #G=(5,4,5)
# #stimiasmo il modello migliore e sul test set forniamo la precisione tramite accuracy e la confusion matrix
# 
# set.seed(123)
# mod_mda_k3<-MclustDA(data_train[,1:4],data_train$fetal_health,G=c(5,4,5),modelNames="VII")
# etichette_prediction_MDA_cv<-predict(mod_mda_k3, select(data_test,-fetal_health))$class
# confusionMatrix(etichette_prediction_MDA_cv, data_test$fetal_health) 

```
Successivamente abbiamo implementato una funzione che valuta il miglior modello mda tramite cross validation con nfold=4 che ci permette di ottenere la seguente confusion matrix:
N.B. la stima del modello tramite cross validation richiede una potenza computazionale non indifferente soprattutto se i modelli da testare e il numero di cluster è molto elevato, per questo motio abbiamo optato per una funzione che imponesse come vincolo che il modello utilizzato in ciascuna delle classi fosse di tipo VII da 1 a 5 gruppi (empiricamente si è visto essere molto accurato ma con un numero di parametri decisamente inferiore (trade-off tra precisione del modello e prestazioni del pc richieste)).
```{r}
set.seed(123)
mod_mda_k3<-MclustDA(data_train[,1:4],data_train$fetal_health,G=c(5,4,5),modelNames="VII")
etichette_prediction_MDA_cv<-predict(mod_mda_k3, select(data_test,-fetal_health))$class
MDA_Confusion_cv <- confusionMatrix(etichette_prediction_MDA_cv,as_factor(pull(data_test,fetal_health)))
MDA_Confusion_cv$table
```
Tramite il modello stimato con cross validation siamo riusciti a migliorare la classificazione dei sospetti a discapito di quella dei patologici. (sentitivity più equilibrate)
Il modello così ottenuto trova difficoltà sempre nella classificazione delle 2 classi con numerosità campionaria inferiore per cui si può ipotizzare di implementare un modello mda caratterizzato da alcune tecniche di oversampling e undersampling.

#### 5.4) Smote 2.0 
Proviamo a valutare i risultati ottenuti effettuando le analisi sul dataset dopo aver bilanciato le classi. A tale scopo ci serviamo di SMOTE, un algoritmo che genera sinteticamente nuovi esempi per
le classi minoritarie, identificando i K-nearest neighbors (K-vicini prossimi) e creando combinazioni
lineari tra le osservazioni esistenti. Questo processo migliora la rappresentazione della classe meno frequente nel dataset, aiutando i modelli di classificazione poichè li espone a più esempi di classi meno rappresentate cosìcche la classificazione diventi più accurata.

Nel nostro caso applichiamo l'algoritmo smote per allenare il modello con 600 osservazioni per la classe dei patologici (quella di nostro interesse) 300 per la classe dei "sospetti" e facciamo undersampling in modo che il classificatore venga allenato su 600 "Normali". Inoltre utilizziamo il numero di "nearest neighbor" di default che è `k = 5`. 
Così facendo otteniamo un modello la cui confusion matrix presenta un minor numero di casi patologici e dubbiosi mal classificati. Oltretutto il rateo di patologici classificato come normale è calato drasticamente, il che è auspicabile rispetto a classificare i patologici come sani, e cioè non procedere con un taglio cesareo o con l'ossigenazione del feto quando necessario.

```{r , echo = F}
#install.packages("scutr")
library(scutr)

dt <- as.data.frame(data_train)
data_new_patologici  <-oversample_smote(dt, "Patologico" , cls_col = "fetal_health", m = 600)
data_new_sospetto    <-oversample_smote(dt, "Sospetto" , cls_col = "fetal_health", m = 300)

new_train<-data_train %>%
  filter(fetal_health == "Normale") %>%
  sample_n(size=300) %>%
  rbind(tibble(data_new_patologici),tibble(data_new_sospetto) ) %>%
  as.data.frame()

mm <-mixmodGaussianModel(family = "all",
                         free.proportions = F) #modello in cui i pj non sono stimati siccome vengono imposti pari a circa 0.5 dall'azione
#di oversampling e undersampling    
set.seed(123)
mod = MclustDA(new_train[,-5],
               new_train$fetal_health ,G=1:5,
               models=mm , verbose = F)


etichette_prediction_oversampling <- predict(mod, select(data_test,-fetal_health))$class

# modsmote <- mixmodLearn(new_train[,-5], new_train$fetal_health ,models=mm,
#                         criterion = "CV")
# 
# PREDICTION<- mixmodPredict(data = select(data_test,-fetal_health), classificationRule=modsmote["bestResult"])
real_labels <- as.factor(data_test$fetal_health)

#etichette_prediction_oversampling<-as.factor(PREDICTION@partition)
#levels(etichette_prediction_oversampling)<-c("Normale","Sospetto","Patologico")

SMOTE_Confusion<-confusionMatrix(etichette_prediction_oversampling,real_labels) 
SMOTE_Confusion$table
```
Questi miglioramenti sono ulteriormente confermati dall'aumento della sensitivity del caso patologico e del caso sospetto che passano rispettivamente da 0.63 a 0.8 e da 0.54 a 0.66, per cui è evidente che siamo più accurati nell'identificare i casi anormali, mantenendo comunque una elevata specificity anche al calare della numerosità della classe normale. Altresì marcata è la diminuzione della sensitivity dei casi normali che è accettabile di fronte ad un discreto aumento in quelle delle classi anormali.
Da ricordare il trade-off che si fa quando vengono utlizzate tecniche di oversampling/undersampling e cioè quello di ridurre la sensitivity della classe maggioritaria per aumentare le sensibilità delle classi minoritarie (dunque aumentando anche la specificity della maggioritaria).
(In altre parole se poniamo come ipotesi nulla l' "essere di classe maggioritaria", quello che facciamo è ridurre l'errore di primo tipo e aumentare l'errore di secondo) parte da controllare
```{r,  echo = F}
SMOTE_Confusion$byClass[,1:2]
```

Dal grafico vediamo come le unità statistiche patologiche son ben distinguibili anche se distanti dal baricentro della classe contenente i casi patologici. 
```{r,fig.align='center', fig.width=5,fig.height=3.5}
prob.post_incertezza<- tibble(predict(mod, select(data_test,-fetal_health))$z) %>%
  rowwise() %>% # operiamo riga per riga
  mutate(incertezza = 1 - max(c_across(everything()))) 

data_test %>%
  ggplot(mapping = aes(x=histogram_mean , y = histogram_max, color = real_labels)) +
  geom_point(size=prob.post_incertezza$incertezza*7)+
  geom_point(data = filter(data_test,etichette_prediction_oversampling != real_labels), 
             color = "black", alpha = 0.3,size=prob.post_incertezza$incertezza[etichette_prediction_oversampling != real_labels]*7) + 
  labs( title = "Classificazione tramite smote")+
  scale_color_manual(values = c("Normale" = "#66FF66", "Sospetto" = "dodgerblue3", "Patologico" = "red"))+
theme(plot.title = element_text(hjust = 0.5,face = "bold", size = 14,margin = margin(10, 0, 20, 0)))  # hjust Imposta l'allineamento orizzontale al centro
          # margin Imposta i margini intorno al grafico
```


### 6) Discussione finale
In definitva si può affermare che la classe dei casi sospetti è molto difficile da definire sia nei modelli di clustering che nei modelli di classification. 
Abbiamo anche osservato come i modelli di tipo MDA sono più accurati dei modelli EDDA probabilmente a causa della presenza di sottogruppi in ciascuna classe. 
Tramite l'utilizzo di SMOTE, di un undersampling e un di classificatore MDA otteniamo un risultato migliore per le classi minoritarie, ma ci appare evidente che lo scarso numero di osservazioni e l'accuratezza  relativamente bassa rendano il nostro classificatore non attendibile per applicazioni mediche. Questi limiti intrinseci del nostro classificatore potrebbero essere superati ricorrendo a tecniche di machine learning più sofisticate che oltre a classificare meglio le classi mantengono un certo grado di interpretabilità dei risultati, fondamentale per fornire risposte giustificate ai medici e rassicurare i pazienti.

<div class="tocify-extend-page" data-unique="tocify-extend-page" style="height: 0;"></div>
